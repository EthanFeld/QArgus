

[[PAGE 1]]
ACCELERATINGINFERENCE FORMULTILAYERNEURAL
NETWORKS WITHQUANTUMCOMPUTERS
Arthur G. Rattew1, 3,∗, Po-Wei Huang2, 3, Naixu Guo4, Lirandë Pira4, Patrick Rebentrost4,5
1Department of Materials, University of Oxford, Oxford OX1 3PH, United Kingdom
2Mathematical Institute, University of Oxford, Oxford OX2 6GG, United Kingdom
3Quantum Motion, 9 Sterling Way, London N7 9HJ, United Kingdom
4Centre for Quantum Technologies, National University of Singapore, Singapore 117543
5Department of Computer Science, National University of Singapore, Singapore 117417
ABSTRACT
Fault-tolerant Quantum Processing Units (QPUs) promise to deliver exponential
speed-ups in select computational tasks, yet their integration into modern deep
learning pipelines remains unclear. In this work, we take a step towards bridging
this gap by presenting the first fully-coherent quantum implementation of a multi-
layer neural network with non-linear activation functions. Our constructions mirror
widely used deep learning architectures based on ResNet, and consist of residual
blocks with multi-filter 2D convolutions, sigmoid activations, skip-connections,
and layer normalizations. We analyse the complexity of inference for networks
under three quantum data access regimes. Without any assumptions, we establish a
quadratic speedup over classical methods for shallow bilinear-style networks. With
efficient quantum access to the weights, we obtain a quartic speedup over classical
methods. With efficient quantum access to both the inputs and the network weights,
we prove that a network with an N-dimensional vectorized input, kresidual block
layers, and a final residual-linear-pooling layer can be implemented with an error
ofϵwithO(polylog(N/ϵ)k)inference cost.
1 INTRODUCTION
Within the past decade, deep learning methods [ 1,2] have become the mainstream methodology
to tackling problems in machine learning and generative artificial intelligence, including tasks in
computer vision [ 3,4,5], natural language processing [ 6,7] and various other tasks with increasing
applicability [ 8,9,10]. This progress is partly facilitated by advances in GPUs, which offer speed-ups
for parallelizable operations such as matrix-vector arithmetic. However, as we approach the physical
limits of Moore’s law [ 11], the continuous upscaling of CPUs and GPUs may begin to plateau.
Consequently, a natural question is whether quantum computing [ 12,13,14] and potential quantum
processing units (QPUs) can offer further acceleration for deep learning.
The field of quantum machine learning (QML) [ 15,16,17], investigates this possibility. QML can
broadly be separated into two main paradigms: (1) quantum algorithms tailored to the structure of
near-term quantum hardware [ 18] under assumptions of limited quantum resources, and (2) using
quantum subroutines to obtain provable speed-ups for existing machine learning models, typical
requiring large amounts of quantum resources necessitating error-corrected fault-tolerant quantum
computers.
In the first paradigm, proposals of quantum neural networks (QNN) based on variational quantum
algorithms (VQA) [ 19,20] train parametrized quantum circuits (PQC) [ 21] in an analogue to multi-
layer neural networks. However, these algorithms face trainability issues in the form of poor
local minima [ 22,23] and vanishing gradients, orbarren plateaus[ 24,25]. Moreover, techniques
mitigating these issues often result in the algorithms being classically simulable [ 26,27]. While
alternate approaches such as quantum kernel methods [ 28,29] and others have been proposed [ 30,31],
they often face similar trainability issues [32, 33].
∗arthur.rattew.science@gmail.com
1arXiv:2510.07195v1  [quant-ph]  8 Oct 2025

[[PAGE 2]]
ErfNormalizeFlattenSquareLinearL2 PoolNormalizeSquare“Cat”2D  ConvNormalizeErfNormalizeFlattenSquareLinearL2 PoolNormalizeSquare“Cat”2D  ConvNormalizeNeural Net
…d×
Concatenate(b)(a)L2 PoolNormalizeSquare“Cat”(c)…d×k×k×ErfNormalizeFlatten2D  ConvNormalizek×Neural Net
Neural Net
Neural Net
ErfNormalizeFlattenSquareLinearL2 PoolNormalizeSquare“Cat”2D  ConvNormalizeErfNormalizeFlattenSquareLinearL2 PoolNormalizeSquare“Cat”2D  ConvNormalizeNeural Net
…d×Concatenate(b)(a)L2 PoolNormalizeSquare“Cat”(c)…d×k×k×ErfNormalizeFlatten2D  ConvNormalizek×Neural Net
Neural Net
Neural Net
Figure 1:Architecture for Convolutional Neural Networks.This figure shows the architectures we
consider with provable quantum complexity guarantees for inference under three regimes of quantum
data access assumptions. (a) Depicts the architecture where both the inputs and network weights are
provided in an efficient quantum data structure. (b) Only the network weights are provided in an
efficient quantum data structure. (c) No input assumptions are made. In all architectures, the input is
assumed to be a rank-3 tensor (e.g., images with 4 channels).
The second paradigm focuses on the use of quantum subroutines [ 34,35,36,37] to provide asymp-
totic speed-ups in the underlying linear algebra of classical machine learning models, e.g., in matrix
inversion, matrix-vector arithmetic, and sampling. Applications include support vector machines [ 38],
regression [ 39], feedforward neural networks [ 40], convolutional neural networks [ 41], transform-
ers [42], and other models [ 43,44,45,46,47,48,49,50,51]. Other works have also explored
speeding up classical neural network training and inference [52, 53, 54].
Main Contributions.In this paper, we propose a method that can be used to accelerate inference
for multilayer residual networks (ResNets) [ 3] on quantum computers, given their significance in
enabling deep networks [ 55,56]. We provide core quantum subroutines and techniques for regularized
multi-filter 2D convolutions, sigmoid activations, skip-connections, and layer normalizations – all of
which we show can be coherently implemented on quantum computers. We list the main contributions
as follows.
•In Section 2, we further develop a modular vector-encoding framework for quantum matrix-vector
arithmetic. This is a special case of quantum block-encodings, with many useful properties.
•In Section 2.3, we derive a novel quantum algorithm for the multiplication of arbitrary full-rank and
dense matrices with the element-wise square of a given vector,withoutincurring a rank-dependence.
To the best of our knowledge, this is the first result which allows a quantum algorithm to utilize an
arbitrary full-rank and dense matrix without a Frobenius norm complexity dependence.
• In Section 2.4, we provide a novel QRAM-free block-encoding for 2D multi-filter convolutions.
•In Section 4, to the best of our knowledge, we derive thefirst coherent quantum implementations
of multi-layer neural networks with non-linear activations. We provide rigorous end-to-end
complexity proofs for inference under three QRAM regimes:
–Regime 1 (inputs and weights provided via QRAM):Assuming QRAM access to both inputs
and weights, for a network with knon-linear activations acting on N-dimensional inputs we
prove ˜O(polylog(N/ϵ)k)inference cost. Moreover, we argue that existing techniques are
insufficient to dequantize this result.
–Regime 2 (weights provided via QRAM):When a cost linear in the dimension of the input
must be paid (i.e., no QRAM for the input), but the network weights are stored in QRAM, we
prove a quartic speedup over exact classical implementations for shallow architectures.
2

[[PAGE 3]]
Architecture Coherent
Multi-
LayerCoherent
Non-
LinearityQRAM-
FreeNorm
Preser-
vationPolylog
1/ϵPolylog
N
Cong et al. [57]∗ CNN Inspired
PQC✗ ✗ ✓ ✓N/A N/A
Allcock et al. [40] Feed-forward✗ ✗ ✗ ✗ ✗ ✗
Kerenidis et al. [41] CNN✗ ✗ ✗ ✗ ✗ ✗
Guo et al. [42] Transformer✗ ✓ ✗ ✗ ✗ ✗
Our work - Regime 1 Residual CNN✓ ✓ ✗ ✓ ✓ ✓
Our work - Regime 2Bilinear
Residual CNN✓ ✓ ✗ ✓ ✓ ✗
Our work - Regime 3Bilinear
Residual CNN✓ ✓ ✓ ✓ ✓ ✗
Table 1:Comparison with prior work.We briefly explain the meaning of each column. Coherent
multi-layer refers to the construction of multi-layer architectures separated by non-linear activation
functions without tomography. Coherent non-linearity refers to the implementation of non-linear
transformations on the quantum computer without readout. Norm preservation refers to the preser-
vation of vector norms throughout the network forward pass. Next, each quantum implementation
of a classical architecture incurs some error over the exact classical implementation, and as such an
entry✓in the polylog 1/ϵcolumn indicates a O(polylog(1/ϵ)) error-dependence, whilst a ✗entry
indicates a O(poly(1/ϵ)) error-dependence. Finally, polylog Nrefers to polylogarithmic complexity
in the input dimension N.∗Note:the architecture presented in Cong et al. [57], is inspired by CNNs
but is based on parameterized quantum circuits (PQC). As they do not aim to accelerate an existing
classical architecture, it is not possible to provide an entry in the polylog ϵcolumn. Moreover, they
do not provide complexities when considering classical input data, and so we do not give an entry in
the column corresponding to polylogN.
–Regime 3 (no QRAM):In the absence of any QRAM, we prove a quadratic speedup over an
exact classical implementation.
The relevant architectures in each regime can be seen in Figure 1. We derive a number of techniques
and algorithms which have broad utility in implementing machine learning architectures on quantum
computers. However, our main focus is on accelerating inference for classification, with our formal
problem statement given in Definition 1.
Definition 1(The Approximate Sampling-Based Classification Problem).Given a neural network
represented by functionh:RD7→RC(i.e. withD-dimensional inputs andC-dimensional outputs)
which returns a probability distribution as its output (i.e., for any x∈RD,y:=h(x) is all non-
negative, and∥y∥1= 1), then the sampling-based classification problem is to return a sample from
some probability vectorˆysuch that∥y−ˆy∥2≤ϵ.
Comparison to Prior Work.In prior work, to achieve multi-layer architectures in feedforward
and convolutional neural networks as well as transformers, intermediate measurements for inner
products [ 40] or quantum state tomography that read out the entire state [ 41,42] are required to
extract information out to classical computers where data is required to be re-encoded into the
quantum circuit for computation in the next layer, breaking the coherence of the quantum architecture
and limiting potential speed-ups. We compare against the prior work in Table 1. To the best of our
knowledge,our work provides the first fully coherent quantum implementation of classical multi-layer
neural networks .Further, our work is also the first in works that accelerate classical deep learning
algorithms to present an architecture which does not use QRAM. Moreover, we demonstrate that
careful tracking on bounds of the vector norm (as it propagates through the forward-pass of a given
network) is required to prevent arbitrary decay of the norm in multilayer structures, and subsequent
unbounded runtimes. We provide rigorous proofs and develop tools to prove this norm preservation
in our architectural blocks. Further, we make the observation that residual skip connections that
enable deep networks classically are fundamental to the norm stability and preservation, enabling us
to provide an efficient and coherent multilayer architecture not present in prior work.
3

[[PAGE 4]]
Introduction to Quantum Computing.Quantum computation can provide asymptotic speed-ups
over their classical counterparts [ 14] by utilizing quantum phenomena. Quantum bits, or qubits,
form the basic unit for computation, and can host a superposition of states expressed as a two-
dimensional complex vector (or ket) |ψ⟩=α|0⟩+β|1⟩ where |α|2+|β|2= 1. With nqubits, we
can create a superposed state over 2nbit strings |i⟩, each with a different amplitude and expressed as
|ψ⟩=P2n−1
i=0vi|i⟩, whereP2n−1
i=0|vi|2= 1. That is, an n-qubit quantum state is a 2n-dimensional
ℓ2-normalized complex vector. Quantum computers achieve computation through applying a circuit
consisting of one-or-two-qubit logical gates [13] on qubits. Quantum circuits can be contracted and
represented as a single unitary operation.
Notation.We use standard big and small Onotations for asymptotics, using ˜Oto hide polylogarith-
mic factors. The notation [N]represents the set of integers 0, ..., N−1 . We use kets to represent
arbitrary (not necessarily normalized vectors). Logarithms are assumed to be base-2 unless otherwise
stated. The subscript on the ket denotes the number of qubits it acts on (i.e., the log of the dimension),
thus|ψ⟩n∈C2n. When we assume a ket is normalized, we will explicitly state that it is. The one
exception is with the definition of a vector-encoding (as defined subsequently in Definition 3). For
example, an (1, a, ϵ) -VE for |ψ⟩nimplicitly implies that ∥|ψ⟩ n∥2= 1, and so we will not explicitly
state the normalization of the encoded vector every time we introduce a VE. A bra is defined as
the conjugate transpose of a ket, ⟨ψ|n=|ψ⟩†
n. We use the notation Into refer to an n-qubit (i.e.,
2n-dimensional) identity matrix. We define the Kronecker product with the symbol ⊗, and will
sometimes refer to this as a tensor product.
2 QUANTUMMATRIX-VECTORARITHMETIC
In this section, we define and motivate the tools necessary to perform quantum matrix-vector
arithmetic. These subroutines are essential for our subsequent results implementing classical neural
networks on quantum computers. In Section 2.1, we provide a summary of quantum block-encodings
and quantum vector encodings.Novel contributions in this section:In Section 2.2, we further
develop the framework of vector-encodings, introducing straight-forward new quantum algorithms
for vectors encoded as VEs, enabling vector sums, matrix-vector products, tensor products, and vector
concatenations. In Section 2.3, we present a novel algorithm which applies an arbitrary full-rank and
dense matrix to the element-wise square of a vector, without incurring a Frobenius norm dependence.
Finally, in Section 2.4, we give a novel QRAM-free block-encoding for 2D multi-filter convolutions.
2.1 QUANTUMBLOCK-ENCODINGS ANDVECTOR-ENCODINGS
A widely used tool in quantum algorithm design is the block-encoding [ 36], which can be viewed as a
way to encode and manipulate matrices in quantum algorithms. A block-encoding is a unitary matrix
U, specified by a quantum circuit, whose top left block contains a matrix ˜A(such that ∥˜A∥2≤1)
which is a scaled approximation to some matrixA. We give the formal definition in the following.
Definition 2(Block encoding [ 36]).Suppose that Ais a2s×2smatrix, α, ϵ∈R +anda∈N , then
we say that the2s+a×2s+aunitary matrixUis an(α, a, ϵ)-block-encoding ofA, if
∥A−α(⟨0|⊗a⊗I)U(|0⟩⊗a⊗I)∥ ≤ϵ.
Essentially, noting that ⟨0|⊗a⊗I= ( I0. . .0 ), we see that ⟨0|⊗a⊗Iselects the first 2srows of
U, and then |0⟩⊗a⊗Iselects the first 2scolumns of (⟨0|⊗a⊗I)U , meaning that (⟨0|⊗a⊗I)U(|0⟩⊗a⊗
I)is simply the top-left 2s×2sblock of U. Indeed, if ϵ= 0 , then A/α= (⟨0|⊗a⊗I)U(|0⟩⊗a⊗I) .
Additionally, αcan be viewed as an upper-bound on the normalization factor of A, e.g., if ϵ= 0 , then
∥A/α∥2≤1. Any matrix encoded in a sub-block of a unitary matrix cannot have norm exceeding 1.
Analogously to how a quantum block-encoding encodes a general matrix in the top left block of a
unitary, we can embed arbitrary (sub-normalized) N-dimensional vectors in the first Nrows of a
larger vector corresponding to a normalized quantum state.
This naturally leads to the following definition of quantum vector-encodings (VEs), the definition of
which we take nearly verbatim from Rattew and Rebentrost [58], where they were called SPBEs.
4

[[PAGE 5]]
Definition 3(Vector-Encoding (VE) [ 58]).Let α≥1 ,a∈N , and ϵ≥0 . We call the 2a+n×2a+n
unitary matrixU ψan(α, a, ϵ)−VE for the2n-dimensional quantum state|ψ⟩ n, if
∥|ψ⟩ n−α(⟨0| a⊗In)Uψ|0⟩a+n∥2≤ϵ.(1)
Note that (⟨0|a⊗In)Uψ|0⟩a+ncorresponds to the exact vector encoded by Uψ, specifically encoded
in the first 2nrows of the first column of Uψ. The parameter αis a measure of the norm of the encoded
vector, e.g., if ϵ= 0 then∥(⟨0| a⊗In)Uψ|0⟩a+n∥2= 1/α . One of the most essential components of
working with matrix-vector arithmetic in quantum algorithms is tracking the norm of the encoded
vectors throughout the algorithm, as the quantum complexity is usually inversely proportional to the
norm of the encoded vector. Vector encodings give a methodical way to track encoded vector norms
when implementing various matrix-arithmetic operations on the encoded vectors.
2.2 NEWOPERATIONS ONVECTORENCODINGS
To enable our results on architectural blocks, we had to develop primitive operations on vector-
encodings. These results are straight-forward modifications of existing techniques into the VE
framework, but are necessary to allow easy tracking of the norm of encoded vectors, which is a
crucial parameter dictating the complexity of quantum neural network accelerations.
Lemma 1(Vector Sum, Proof in Appendix B).Let 0≤τ≤1 . We are given unitary circuits Uψand
Uϕwhich are (α, a, ϵ 0)and(β, b, ϵ 1)VEs for |ψ⟩nand|ϕ⟩n, respectively. Define c:= max(a, b) ,
|Γ⟩n:=τ
α|ψ⟩n+(1−τ)
β|ϕ⟩n,N:=∥|Γ⟩ n∥2and|Γ⟩n:=|Γ⟩ n/N. Then, using one controlled Uψ
circuit, one controlledU ϕcircuit, and two additional single-qubit gates, we can construct a unitary
matrixVsuch thatVis a(N−1, c+ 1,(ϵ0
α+ϵ1
β)/N)-VE for| Γ⟩.
Lemma 2(Matrix-Vector Product, Proof in Appendix B).We are given an (α, a, ϵ 0)-block-encoding
UAfor the n-qubit operator A, and Uψa(β, b, ϵ 1)-VE for the ℓ2-normalized n-qubit quantum state
|ψ⟩. LetN:=∥A|ψ⟩ n∥2.UψhasTψcircuit complexity, and UAhasTAcircuit complexity. Then,
we can obtain an a+b+n qubit unitary UwithO(Tψ+TA)circuit complexity such that Uis an
(αβ/N, a+b,(ϵ 0+αϵ 1)/N)-VE for the quantum state stateA|ψ⟩ n/N.
Lemma 3(Tensor Product of Vector Encodings, Proof in Appendix B).Given Uψan(α, a, ϵ) -VE for
|ψ⟩nwithO(Tψ)circuit complexity, and Uϕan(β, b, δ) -VE for |ϕ⟩mwithO(Tϕ)circuit complexity,
then we can obtain the circuit Vwhich is an (αβ, a+b, ϵ+δ+ϵδ) -VE for |ψ⟩n⊗ |ϕ⟩ mwith
O(max(T ψ, Tϕ) + max(n, b))circuit depth.
Lemma 4(Concatenation of Vector Encodings, Proof in Appendix B).Let D= 2d,N= 2n, and
0≤ϵ <1 . Assume that d≤n . Suppose we are given a set of Dunitary circuits, {Ui}i∈[d] such
that each Uiis an (αi, a, ϵ) -VE for the quantum state |ψi⟩nwithO(T) circuit complexity.1Let
|Ψ⟩d+n=PD−1
j=0|j⟩d|ψj⟩/αj, and let N:=∥|Ψ⟩ d+n∥2=qPD−1
j=01
α2
j. Then, we can obtain a
(D/N, d+a, ϵ)for|Ψ⟩d+n
NwithO(dDT)circuit complexity.
2.3 MATRIXVECTORSQUAREDPRODUCT
We are now ready to present the first key result of this section, showing how given a matrix W(with
∥W∥2≤1) and a vector encoding of x, we can obtain a vector encoding of W(x)2. The key idea is
to avoid obtaining a quantum block-encoding of the operator W(which in general requires Wto be
either low-rank, or sparse [ 36]). We then implement the product by using importance-weighting to
coherently combine the columns of Wweighted by the corresponding elements of the input vector,
and then apply the result to a modified version of the input vector.
Theorem 1(Product of Arbitrary Matrix with a Vector Element-wise Squared, Informal).Let
N= 2n. We are given a matrix W∈CN×N, provided via a pre-processed efficient quantum
accessible data-structure. Additionally, we are given the unitary Uψwith circuit complexity O(Tψ),
a(α, a, ϵ) -VE for the quantum state |ψ⟩n. Define the function g:C7→R asg(x) =|x|2, and
N:=∥Wg(|ψ⟩ n)∥2. Then we can construct the unitary Ufwhich is a (α2
N,2a+ 2n+ 3,2αϵ
N)-VE
forWg(|ψ⟩ n)/N, and hasO(T ψ+n2)circuit depth.2
1IfDis not a power of2, padding can be added.
2For simplicity, here we are assuming that the parameterd(as defined in Theorem B.1) is set ton.
5

[[PAGE 6]]
Figure 2:Generic Residual Architectural Block.This diagram illustrates the structure of a typical
residual block used in deep neural networks. The input vector xis transformed through a sequence
of operations: a learnable linear transformation W, a non-linear activation function f, and a residual
(skip) connection that adds the original input to the transformed signal. The output is then passed
through a normalization layer (norm).
This result is stated formally and proven as Theorem B.1 in the Appendix, and we formally define one
possible implementation of the quantum accessible data-structure assumption in Definition B.3. To use
this to prepare the quantum state Wg(|ψ⟩ n)/N, the vector normalization result (Lemma B.8) can be
directly applied to the output VE yielded by Theorem 1, preparing the state with ˜O(α2(Tψ+n2)/N)
circuit complexity. This is the first such resultwithout a Frobenius norm dependenceonA.
We will now informally sketch the proof of this procedure. First, define the columns of Was
W= ( w0. . .w N−1). Define the normalized version as |wj⟩=w j/∥w j∥2, and define aj:=
∥wj∥2. We assume access to three objects. (1) A block-encoding of A:=diag(a 0, . . . , a N1).
(2) An oracle implementing UW|0⟩|j⟩=|w j⟩|j⟩. (3) A vector-encoding for |ψ⟩=P
jψj|j⟩.
Then, by using our vector-encoding circuit, we can get an encoding of |ϕ⟩:=P
jψj|j⟩|w j⟩=
(ψ0⟨w0|. . . ψ N−1⟨w1|)†. Then, using our block-encoding of A, we can efficiently get a block-
encoding of
a0ψ0In. . . a N1ψN−1In
0
(where Inis a2ndimensional identity matrix, and
only the first Nrows are non-zero). We can then use the product of matrix-encoding with vector
encoding result to take the product of
a0ψ0In. . . a N1ψN−1In
0
with|ϕ⟩yielding the desired
vector-encoding.
2.4 QRAM-FREEQUANTUMENCODING OF2D MULTI-FILTERCONVOLUTIONS
While the matrix-form of a 2Dconvolution has been given many times before in the literature, to the
best of our knowledge the following is the first result giving a block-encoding of a QRAM-free 2D
multi-filter convolution. We also stress that the following result can be highly optimized, especially if
QRAM is used. We leave such optimizations to future work. The full proof is provided in Section B.2.
Lemma 5(QRAM-Free Block-Encoding of 2D Convolution With Filters).Let M= 2m, let
n= 2m , letN= 2n, and let D= 2d. Define the matrix form of the 2D multi-filter convolution
operation, C ∈RCM2×CM2, as per Lemma B.17. Here, Crepresents the number of input and
output channels, and Drepresents the dimension of the kernel over rows and columns (i.e., the
kernel is a rank −4tensor containing C,C×D×D filters). Then, after performing some one-time
classical pre-computation, we can obtain a (1,3 + 8D+ 2 log(CD),0) - block-encoding ofC
2∥C∥2
withO(m2C3D4log(C) log(D))circuit depth.
While the degrees on the number of channels and the filter size Dseem large, the filter size is usually
quite small in practice (e.g., often 3). Moreover, there are straight-forward optimizations of this result
which can substantially reduce the degrees on bothCandD.
3 ARCHITECTURALBLOCKS
In this section we will derive two key architectural block, a residual block, and a multi-layer residual
block, which allow our subsequent complexity claims. We present an additional architectural block
building on these in Appendix C, but do not include it in the main text as it is not essential for
understanding the key complexity details of such quantum implementations.
6

[[PAGE 7]]
Lemma 6(General Skip Norm Block).Let ϵ1∈(0,1] . Let κ∈[1,2] . Consider the architecture
shown in Figure 2. Let N= 2n. We are given the unitary Uψa(1, a, ϵ 0)-VE for |ψ⟩nwith circuit
complexity O(T 1), and are given the unitary UWa(1, b,0) -block-encoding for the n-qubit operator
W/κ with circuit complexity O(T 2)such that ∥W∥2≤1. Define f(x) := erf(4x/5) ,|ψf⟩n:=
|ψ⟩n+f(W|ψ⟩ n), andN:=∥|ψ f⟩n∥2. Then, we can obtain a (1,2(a+b)+n+9,712(ϵ 0+ϵ1))-VE
for|ψ f⟩n/Nwith circuit complexityO(log(√
N
ϵ1) log(1
ϵ1)(a+b+n+T 1+T2)).3
The rigorous proof of this result is provided in Appendix C, but it essentially follows from using
our preceding results on matrix-vector multiplication, vector sums, and the extant results on layer
normalization and applications of the error-function. The key insight enabling this proof is that in
a residual block such as the one we have described, the forward norm of the vector is efficiently
lower-bounded prior to every normalization layer. Without such skip connection, and the techniques
we developed for working with vector-encodings (which enable effective tracking of the norm of
a vector propagating through a network), the norm at the end of such a block could be arbitrarily
small, leading to complexities which could be on the order of ≈Nk(or even unbounded) for k-layer
architectures – completely intractable even for constant depth networks. As a consequence, we are
able to prove the following result for multi-layer residual blocks.
Lemma 7(Sequence of kResidual Blocks).Let N= 2n. Suppose we are given a unitary Uψwith
circuit complexity O(T 1)such that it is a (1, a,0) -VE for |ψ⟩n. Let kbe an asymptotic constant.
Suppose we have a sequence of kresidual blocks (as per Lemma 6), with weight layers implemented by
kunitaries {UWi}isuch that UWi(with circuit complexity O(T 2)) is a(1, b,0) -block-encoding for the
n-qubit operator Wi/2, and for all i,∥Wi∥2≤1. Then, we can prepare a (1,2k(a+ 2b+n+ 9), ϵ) -
VE for the output of the kresidual blocks with O(log(√
N/ϵ)2k(a+ 2b+n+T 1+T2))circuit
depth.
This result is proven in Appendix C, and follows by repeatedly invoking Lemma 6 with its output as
the next input. It appears that the complexity of this result as a function of the number of layers kis a
fundamental limitation of any quantum algorithm. As described in greater detail in Appendix C, for a
unitary matrix (a linear operator) to enact a non-linear transformation on a vector, its definition must
in general be input-dependent. Consequently, unless Lemma 6 can be implemented with only a single
copy of its input, it seems unlikely that this complexity can be avoided. This suggests that quantum
computers are best suited for accelerating the wide and shallow regime, which is a popular regime for
classical inference accelerators (since wide networks can be parallelized on classical hardware, but
depth cannot be parallelized). Classically, with the aim of accelerating both inference and training,
there are a range of techniques for compressing neural networks [ 59]. Moreover, classically, deep
neural networks are much harder to accelerate than their shallow and wide counterparts (you can
parallelize matrix-multiplications, but not consecutive layers). Consequently, there are a number of
classical architectures striving for shallow networks (e.g. Zagoruyko and Komodakis [60]) which can
serve as sources of inspiration for designing architectures best suited for quantum acceleration. We
discuss this in greater detail in Appendix C.
4 ARCHITECTURES
We will now use the architectural blocks which we have derived to prove the quantum complexity in
inference for the architectures shown in Figure 1 (a), which is then used to prove the complexity of
the architecture in panel (b), and a corollary is used to prove the complexity of the architecture in
panel (c).
Our results in all three data-access regimes all following from the following general result.
Theorem 2(General Multilayer Convolutional Network with Skip Connections).Let M= 2m,
N= 2n=M2. Consider the neural network architecture shown in Figure 1 (a). Let the input Xbe a
rank−3tensor of dimension 4×M×M (with an R, G, B and null channel, where the null channel has
all0s). Assume that ∥vec(X)∥2= 1, and that we have access to a unitary UXthat is a (1,0,0) -VE
for the input in column-major layout |X⟩2+2m =P4
i=0PM−1
j=0PM−1
k=0Xi,k,j|i⟩2|j⟩m|k⟩m. Assume
thatUXhasO(TX)circuit complexity. As shown in the figure, we have a sequence of kresidual
3We implicitly assume that ∥W|ψ⟩ n∥>0 , which is a reasonable assumption for any input which comes
from the same distribution as the training data.
7

[[PAGE 8]]
convolutional layers, where each convolutional layer has 16input channels, 16output channels (i.e.,
16filters) with filter width and height 3. I.e., each convolutional layer has 16×16×3×3 = 2304
parameters. Assume that there is 0padding so the input and outputs always have the same dimension,
and that there is a stride of 1. Suppose each convolutional layer has been regularized, so that its
spectral norm is at most 1. Let Wrepresent the N×N full-rank linear layer applied in the final output
block of the network, and assume that ∥W∥2≤1. LetCrepresent the number of output classes, and
assume that C= 2c(padding can be added otherwise). Let the overall network be represented by the
function h:R4×M×M7→RC. Lety=h(X) (and note that ∥y∥1= 1, and y∈RC). Then, with
O(log(√
N/ϵ)2k+1(TX+n2))total circuit depth, and with O(2kn)ancillary qubits, we can draw a
sample from anℓ 1-normalizedC-dimensional vector ˜ysuch that∥y− ˜y∥2≤ϵ.
Proof. We have a 4channel input and we want to map this to a 16channel in-
put (by concatenating |X⟩2+2m vector with itself 4 times). Let |X⟩4+2m :=
1√
4(⟨X|2+2m ⟨X|2+2m ⟨X|2+2m ⟨X|2+2m )T. We can invoke Lemma 4 with UXfour times,
obtaining a (1,0,0) -VE for |X⟩4+2m withO(TX)circuit complexity. Using Lemma 5, for each
of the i= 0, ..., k−1 convolutions, we can obtain a (1,27,0) -block-encoding for Ci/2∥C i∥2(the
matrix form of the corresponding convolution) with O(m2)circuit depth. Consequently, we can
invoke Lemma 7 to obtain Uconva(1,2k(63 +n), ϵ) -VE for the ℓ2-normalized output of the sequence
ofkresidual blocks. Moreover, UconvhasO(log(√
N/ϵ)2k(n+T X+m2))circuit depth. Then,
we can invoke Lemma C.1 with Uconvto draw a sample from some probability vector ˜y∈RCsuch
that∥˜y−y∥2≤ϵwithO(log(√
N/ϵ)2k+1(TX+n2))total circuit depth and with O(2kn)ancilla
qubits.
4.1 KEYRESULTS UNDERDIFFERINGQUANTUMDATAACCESSASSUMPTIONS
The feasibility of quantum random access memory, the primary method assumed in the literature for
accessing classical data in quantum algorithms, is widely debated in the literature [ 61]. However,
recent work [ 62] provides a promising path forward, addressing many of the limitations raised
in Jaques and Rattew [61]. Regardless, algorithms papers often fail to meaningfully address the
memory assumptions they make, and so we include a comprehensive discussion of it in Appendix D
highlighting the feasibility of the technology, and that importantly our QRAM assumptions are no
stronger than the usual made in such algorithms papers.
4.1.1 REGIME1: INPUT ANDNETWORKUSEQRAM
The primary purpose of the architecture we presented in Regime 1 is to show that quantum computers
can implement multi-layer neural networks based on real architectures coherently, with reasonable
input assumptions, and with cost polylogarithmic in the dimension of the network. As per the main-
text, in this regime we assume that the matrix weights (in particular for the final full-rank linear layer)
and vectorized input are provided via QRAM. The architecture for this regime is shown in Figure 1
(a). Let the dimension of the vectorized input be O(N) . Since the input is provided via QRAM,
TXas defined in Theorem 2 is TX∈O(polylog(N)) (see, Section D.2).Thus, for a constant
number of layers k, the cost to perform inference (in accordance with Definition 1) becomes
O(polylog(√
N/ϵ)k). Please see Section E.1 for a detailed discussion outlining important application
areas where such input assumptions are practical (namely, where the input can be constructed in an
amortized fashion online). Moreover, we also discuss considerations relating the receptive field of
such architectures, and argue that existing techniques are insufficient to dequantize this result.
4.1.2 REGIME2: NETWORKSTORED INQRAM, INPUTLOADEDWITHOUTQRAM
The architecture in this regime is shown in Figure 1 (b). The architecture contains dpaths of purely
classical neural networks, which each operate on O(N) dimensional (vectorized) inputs. These
classical architectures are assumed to have ˜O(N) time complexity in terms of the input. These
separate paths are then normalized, converted to quantum states, and then the Kronecker product
of the result is taken. The result is fed into exactly the same architecture as in Regime 1. This
architecture is inspired by bilinear neural networks [ 63]. Consequently, to determine the cost of this
architecture, we can again invoke Theorem 2. Here, we need to pay an ˜O(N) cost to load each of
8

[[PAGE 9]]
the input paths in as a quantum state (via brute-force [ 64]),TX∈O(N) . Consequently, we obtain
an overall algorithmic complexity of O(Nlog(Nd/2
ϵ)2k), which for constant kandd, simplifies to
˜O(Nlog(1/ϵ)2k). When d= 2 , the dimension after the tensor product is N2. Consequently, the final
linear layer contains a matrix multiplication of an N2×N2matrix with an N2dimensional vector,
which takes Ω(N4)time.Consequently, for a constant k, this architecture produces a quartic
speedup for the inference problem defined in Definition 1 over exact classical computation.
When d= 1 , the speedup due to the final layer is instead quadratic. This speedup can be increased by
settingdto larger values.
4.1.3 REGIME3: NOQRAM
This architecture is identical to the one presented in Regime 2, only dropping the final full-rank
linear block. In Section E.3 we show that the architecture in Figure 1 (c) can perform inference with
a total O(Nlog(1/ϵ)2k)circuit complexity. Since the dimension of the vector acted on by the 2D
convolution is O(N2)(when d=2), the classical cost to compute this is Ω(N2): showinga quadratic
speedup over an exact classical implementation. The speedup can be made asymptotically larger
by increasingd. We have a more detailed discussion of this regime in Section E.3.
5 CONCLUSION
This work proposes a modular framework for accelerating classical deep learning inference using
fault-tolerant quantum subroutines. Our approach offers direct quantum implementations of important
neural network architectural blocks (such as convolutions, activation functions, normalization layers,
and residual connections), and uses structured primitives such as quantum block-encodings.
In summary, we provide a number of novel theoretical contributions. We further develop the VE
framework for quantum vector encodings. We derive a novel quantum algorithm for the multiplication
of an arbitrary dense and full-rank matrix with the element-wise square of a given vector, which to
the best of our knowledge, is the first such result which does not incur a Frobenius norm (and thus
rank) complexity dependence. We provide a novel QRAM-free block-encoding of multi-filter 2D
convolutions. We then prove the first end-to-end complexity guarantees for the coherent quantum
acceleration of multi-layer neural network inference, under three QRAM regimes. In the first regime,
we give complexity which is polylogarithmic in both the dimension of the input, and the number of
parameters in the network. In the second, we show a quartic speedup over exact classical computation.
In the third, we show a quadratic speedup.
6 FUTUREWORK
To the best of our knowledge, this was the first paper to implement multi-layer neural networks
coherently on a quantum computer, and as such, many important open directions of research remain.
Moreover, progress towards achieving a practically passive QRAM is important for realizing the
speedups in the first two regimes. Additionally, future work can explore classical, quantum inspired,
algorithms using the norm-preservation ideas we developed. Moreover, exploring the connection
between this work and the techniques utilized in scientific computing (e.g., quantum differential
equation solvers, finite difference methods, etc [ 65,35,66,67,68,69,70,71,72,73,74,75]) would
be interesting. Most importantly, we wonder if it is possible to coherently enact sequences of non-
linear transformations without an exponentially increasing circuit depth (and with polylogarithmic
error-dependence), thereby allowing very deep multi-layer architectures to be quantized, but we
suspect that this is provably impossible (at least in general). Furthermore, it is conceivable that an
approach enacting the non-linear transformations coherently with techniques based on QPE [ 76] might
be able to enact a sequence of non-linearities without exponentially increasing circuit depth (albeit at
the cost of an exponentially worse and exponentially decaying error-dependency). Combining such
approaches may let quantum computers coherently accelerate architectures with depths of e.g., up to
25.
9

[[PAGE 10]]
AUTHORCONTRIBUTIONS
AGR conceived of the project, led it, and developed the theory. PWH contributed to the design
of the architectures considered, and produced the key figures. AGR and PWH drafted the original
manuscript. PR supervised the project, and provided input on the theory. NG verified the proofs. LP
helped position the paper over prior work. All authors contributed to the final manuscript.
ACKNOWLEDGMENTS
AGR would like to thank Simon Benjamin and Sam Jaques for helpful discussions about QRAM.
AGR acknowledges support from the Engineering and Physical Sciences Research Council (EPSRC)
project Software Enabling Early Quantum Advantage (SEEQA) under grant EP/Y004655/1, and
additionally acknowledges support from Quantum Motion. PWH acknowledges support from the
EPSRC Doctoral Training Partnership (DTP) under grant EP/W524311/1, with a CASE Conversion
Studentship in collaboration with Quantum Motion. PWH further acknowledges support from the
Ministry of Education, Taiwan for a Government Scholarship to Study Abroad (GSSA) and St.
Catherine’s College, University of Oxford for an Alan Tayler Scholarship. NG, LP, and PR are
supported by the National Research Foundation, Singapore, and A*STAR under its CQT Bridging
Grant and its Quantum Engineering Programme under grant NRF2021-QEP2-02-P05. NG also ac-
knowledges support through the Research Excellence Scholarship from SandboxAQ. LP additionally
acknowledges support from the Alice Postdoctoral Fellowship, awarded by the Centre for Quantum
Technologies, National University of Singapore.
REFERENCES
[1]Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning.Nature, 521(7553):
436–444, 2015. URLhttps://doi.org/10.1038/nature14539.
[2]Ian Goodfellow, Yoshua Bengio, and Aaron Courville.Deep Learning. MIT Press, 2016. URL
http://www.deeplearningbook.org.
[3]Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for
image recognition. InProceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pages 770–778, 2016. URLhttps://doi.org/10.1109/CVPR.
2016.90.
[4]Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,Advances in Neural
Information Processing Systems, volume 33, pages 6840–6851. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/
hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
[5]Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image
recognition at scale. InInternational Conference on Learning Representations, 2021. URL
https://openreview.net/forum?id=YicbFdNTTy.
[6]Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,
Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,
2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/
hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
[7]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-
wal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz
Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
10

[[PAGE 11]]
Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,Advances in Neu-
ral Information Processing Systems, volume 33, pages 1877–1901. Curran Associates, Inc.,
2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
[8]David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den
Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot,
Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy
Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Master-
ing the game of Go with deep neural networks and tree search.Nature, 529:484–489, 2016.
URLhttps://doi.org/10.1038/nature16961.
[9]John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-
neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, Alex
Bridgland, Clemens Meyer, Simon A. A. Kohl, Andrew J. Ballard, Andrew Cowie, Bernardino
Romera-Paredes, Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen,
David Reiman, Ellen Clancy, Michal Zielinski, Martin Steinegger, Michalina Pacholska,
Tamas Berghammer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W. Senior,
Koray Kavukcuoglu, Pushmeet Kohli, and Demis Hassabis. Highly accurate protein structure
prediction with AlphaFold.Nature, 596(7873):583–589, July 2021. ISSN 1476-4687. URL
https://doi.org/10.1038/s41586-021-03819-2.
[10] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes,
Mohammadamin Barekatain, Alexander Novikov, Francisco J. R. Ruiz, Julian Schrit-
twieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli. Discov-
ering faster matrix multiplication algorithms with reinforcement learning.Nature, 610
(7930):47–53, October 2022. ISSN 1476-4687. URL https://doi.org/10.1038/
s41586-022-05172-4.
[11] Gordon E. Moore. Cramming more components onto integrated circuits.Electronics,
38(8):114–117, April 1965. URL https://ieeexplore.ieee.org/document/
4785860.
[12] Richard P. Feynman. Simulating physics with computers.International Journal of Theoretical
Physics, 21(6–7):467–488, June 1982. ISSN 1572-9575. URL https://doi.org/10.
1007/BF02650179.
[13] Richard P. Feynman. Quantum mechanical computers.Foundations of Physics, 16(6):507–531,
June 1986. ISSN 1572-9516. URLhttps://doi.org/10.1007/BF01886518.
[14] Michael A. Nielsen and Isaac L. Chuang.Quantum Computation and Quantum Information.
Cambridge University Press, 2010. ISBN 9781107002173. URL https://doi.org/10.
1017/CBO9780511976667.
[15] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth
Lloyd. Quantum machine learning.Nature, 549(7671):195–202, 2016. URL https:
//doi.org/10.1038/nature23474.
[16] M. Schuld and F. Petruccione.Machine Learning with Quantum Computers. Springer
Cham, 2021. ISBN 9783030830984. URL https://doi.org/10.1007/
978-3-030-83098-4.
[17] Yuxuan Du, Xinbiao Wang, Naixu Guo, Zhan Yu, Yang Qian, Kaining Zhang, Min-Hsiu Hsieh,
Patrick Rebentrost, and Dacheng Tao. Quantum machine learning: A hands-on tutorial for
machine learning practitioners and researchers, 2025. URL https://arxiv.org/abs/
2502.01146.
[18] John Preskill. Quantum computing in the NISQ era and beyond.Quantum, 2:79, August 2018.
URLhttps://doi.org/10.22331/q-2018-08-06-79.
11

[[PAGE 12]]
[19] Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J.
Love, Alán Aspuru-Guzik, and Jeremy L. O’Brien. A variational eigenvalue solver on a
photonic quantum processor.Nature Communications, 5:4213, 2014. URLhttps://doi.
org/10.1038/ncomms5213.
[20] Marco Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C. Benjamin, Suguru Endo, Keisuke
Fujii, Jarrod R. McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, and Patrick J. Coles.
Variational quantum algorithms.Nat. Rev. Phys, 3(9):625–644, August 2021. URL http:
//doi.org/10.1038/s42254-021-00348-9.
[21] Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized quantum
circuits as machine learning models.Quantum Sci. Technol., 4(4):043001, November 2019.
URLhttp://doi.org/10.1088/2058-9565/ab4eb5.
[22] Lennart Bittel and Martin Kliesch. Training variational quantum algorithms is NP-hard.Phys.
Rev. Lett., 127:120502, Sep 2021. URL https://link.aps.org/doi/10.1103/
PhysRevLett.127.120502.
[23] Eric R. Anschuetz and Bobak T. Kiani. Quantum variational algorithms are swamped with
traps.Nature Communications, 13(1), December 2022. ISSN 2041-1723. URL https:
//doi.org/10.1038/s41467-022-35364-5.
[24] Jarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush, and Hartmut Neven.
Barren plateaus in quantum neural network training landscapes.Nature Communications, 9
(1), November 2018. URLhttps://doi.org/10.1038/s41467-018-07090-4.
[25] Martín Larocca, Supanut Thanasilp, Samson Wang, Kunal Sharma, Jacob Biamonte, Patrick J.
Coles, Lukasz Cincio, Jarrod R. McClean, Zoë Holmes, and M. Cerezo. Barren plateaus in
variational quantum computing, March 2025. ISSN 2522-5820. URL https://doi.org/
10.1038/s42254-025-00813-9.
[26] M. Cerezo, Martin Larocca, Diego García-Martín, N. L. Diaz, Paolo Braccia, Enrico Fontana,
Manuel S. Rudolph, Pablo Bermejo, Aroosa Ijaz, Supanut Thanasilp, Eric R. Anschuetz, and
Zoë Holmes. Does provable absence of barren plateaus imply classical simulability?Nature
Communications, 16(1), August 2025. ISSN 2041-1723. URL https://doi.org/10.
1038/s41467-025-63099-6.
[27] Pablo Bermejo, Paolo Braccia, Manuel S. Rudolph, Zoë Holmes, Lukasz Cincio, and
M. Cerezo. Quantum convolutional neural networks are (effectively) classically simulable,
2024. URLhttps://arxiv.org/abs/2408.12739.
[28] V ojtˇech Havlí ˇcek, Antonio D. Córcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala,
Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature
spaces.Nature, 567(7747):209–212, March 2019. ISSN 1476-4687. URL https://doi.
org/10.1038/s41586-019-0980-2.
[29] Maria Schuld and Nathan Killoran. Quantum machine learning in feature Hilbert spaces.
Phys. Rev. Lett., 122:040504, February 2019. URL https://link.aps.org/doi/10.
1103/PhysRevLett.122.040504.
[30] Marcello Benedetti, Delfina Garcia-Pintos, Oscar Perdomo, Vicente Leyton-Ortega, Yunseong
Nam, and Alejandro Perdomo-Ortiz. A generative modeling approach for benchmarking
and training shallow quantum circuits.npj Quantum Information, 5(1), May 2019. ISSN
2056-6387. URLhttps://doi.org/10.1038/s41534-019-0157-8.
[31] Po-Wei Huang and Patrick Rebentrost. Post-variational quantum neural networks, 2024. URL
https://arxiv.org/abs/2307.10560.
[32] Supanut Thanasilp, Samson Wang, M. Cerezo, and Zoë Holmes. Exponential concentration in
quantum kernel methods.Nature Communications, 15(1), June 2024. ISSN 2041-1723. URL
https://doi.org/10.1038/s41467-024-49287-w.
12

[[PAGE 13]]
[33] Manuel S. Rudolph, Sacha Lerch, Supanut Thanasilp, Oriel Kiss, Oxana Shaya, Sofia Val-
lecorsa, Michele Grossi, and Zoë Holmes. Trainability barriers and opportunities in quantum
generative modeling.npj Quantum Information, 10(1), November 2024. ISSN 2056-6387.
URLhttps://doi.org/10.1038/s41534-024-00902-0.
[34] Aram W. Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems
of equations.Phys. Rev. Lett., 103:150502, October 2009. URL https://link.aps.
org/doi/10.1103/PhysRevLett.103.150502.
[35] Ashley Montanaro. Quantum algorithms: an overview.npj Quantum Information, 2(1), January
2016. ISSN 2056-6387. URLhttps://doi.org/10.1038/npjqi.2015.23.
[36] András Gilyén, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value
transformation and beyond: exponential improvements for quantum matrix arithmetics. In
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC
2019, pages 193–204, New York, NY , USA, 2019. Association for Computing Machinery.
ISBN 9781450367059. URLhttps://doi.org/10.1145/3313276.3316366.
[37] Alexander M. Dalzell, Sam McArdle, Mario Berta, Przemyslaw Bienias, Chi-Fang Chen,
András Gilyén, Connor T. Hann, Michael J. Kastoryano, Emil T. Khabiboulline, Aleksander
Kubica, Grant Salton, Samson Wang, and Fernando G. S. L. Brandão. Quantum algorithms: A
survey of applications and end-to-end complexities, April 2025. URL https://doi.org/
10.1017/9781009639651.
[38] Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine
for big data classification.Phys. Rev. Lett., 113:130503, September 2014. URL https:
//link.aps.org/doi/10.1103/PhysRevLett.113.130503.
[39] Nathan Wiebe, Daniel Braun, and Seth Lloyd. Quantum algorithm for data fitting.Phys.
Rev. Lett., 109:050505, Aug 2012. URL https://link.aps.org/doi/10.1103/
PhysRevLett.109.050505.
[40] Jonathan Allcock, Chang-Yu Hsieh, Iordanis Kerenidis, and Shengyu Zhang. Quantum
algorithms for feedforward neural networks.ACM Trans. Quantum Comput., 1(1), October
2020. URLhttps://doi.org/10.1145/3411466.
[41] Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum algorithms for deep
convolutional neural networks. InInternational Conference on Learning Representations,
2020. URLhttps://openreview.net/forum?id=Hygab1rKDS.
[42] Naixu Guo, Zhan Yu, Matthew Choi, Aman Agrawal, Kouhei Nakaji, Alán Aspuru-Guzik,
and Patrick Rebentrost. Quantum linear algebra is all you need for Transformer architectures,
2024. URLhttps://arxiv.org/abs/2402.16714.
[43] Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis.
Nature Physics, 10(9):631–633, July 2014. ISSN 1745-2481. URL https://doi.org/
10.1038/nphys3029.
[44] Nathan Wiebe, Ashish Kapoor, and Krysta M. Svore. Quantum deep learning.Quantum
Information and Computation, 16(7 & 8):541–587, May 2016. ISSN 1533-7146. URL
https://doi.org/10.26421/QIC16.7-8-1.
[45] Patrick Rebentrost, Thomas R. Bromley, Christian Weedbrook, and Seth Lloyd. Quantum
Hopfield neural network.Phys. Rev. A, 98:042308, October 2018. URL https://link.
aps.org/doi/10.1103/PhysRevA.98.042308.
[46] Ashish Kapoor, Nathan Wiebe, and Krysta Svore. Quantum perceptron models.
In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors,Ad-
vances in Neural Information Processing Systems, volume 29. Curran Associates,
Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
d47268e9db2e9aa3827bba3afb7ff94a-Abstract.html.
13

[[PAGE 14]]
[47] El Amine Cherrat, Iordanis Kerenidis, Natansh Mathur, Jonas Landman, Martin Strahm, and
Yun Yvonna Li. Quantum vision transformers.Quantum, 8:1265, February 2024. ISSN
2521-327X. URLhttps://doi.org/10.22331/q-2024-02-22-1265.
[48] Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum
speed-up in supervised machine learning.Nature Physics, 17:1013–1017, 2021. URL https:
//doi.org/10.1038/s41567-021-01287-z.
[49] Siyi Yang, Naixu Guo, Miklos Santha, and Patrick Rebentrost. Quantum Alphatron: quantum
advantage for learning with kernels and noise.Quantum, 7:1174, November 2023. ISSN
2521-327X. URLhttps://doi.org/10.22331/q-2023-11-08-1174.
[50] Petr Ivashkov, Po-Wei Huang, Kelvin Koor, Lirandë Pira, and Patrick Rebentrost. QKAN:
Quantum Kolmogorov-Arnold networks, 2024. URL https://arxiv.org/abs/2410.
04435.
[51] Yunfei Wang, Ruoxi Jiang, Yingda Fan, Xiaowei Jia, Jens Eisert, Junyu Liu, and Jin-Peng
Liu. Towards efficient quantum algorithms for diffusion probability models, 2025. URL
https://arxiv.org/abs/2502.14252.
[52] Iordanis Kerenidis and Anupam Prakash. Quantum gradient descent for linear systems
and least squares.Phys. Rev. A, 101(2), February 2020. ISSN 2469-9934. URL https:
//link.aps.org/doi/10.1103/PhysRevA.101.022316.
[53] Amira Abbas, Robbie King, Hsin-Yuan Huang, William J. Huggins, Ramis Movas-
sagh, Dar Gilboa, and Jarrod McClean. On quantum backpropagation, informa-
tion reuse, and cheating measurement collapse. In A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine, editors,Advances in Neural Informa-
tion Processing Systems, volume 36, pages 44792–44819. Curran Associates, Inc.,
2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/
hash/8c3caae2f725c8e2a55ecd600563d172-Abstract.html.
[54] Ji Liu, Mengzhen Liu, Jiapeng Liu, Chengran Peng, Liyuan Zhang, Xiao Yuan, Peng Xue,
and Xiongfeng Ma. Towards provably efficient quantum algorithms for large-scale machine-
learning models.Nature Communications, 15:434, 2024. URL https://doi.org/10.
1038/s41467-023-43957-x.
[55] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. In2017 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 5987–5995, 2017. URL https://doi.org/10.
1109/CVPR.2017.634.
[56] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need:
pure attention loses rank doubly exponentially with depth. In Marina Meila and Tong Zhang,
editors,Proceedings of the 38th International Conference on Machine Learning, volume 139
ofProceedings of Machine Learning Research, pages 2793–2803. PMLR, 18–24 Jul 2021.
URLhttps://proceedings.mlr.press/v139/dong21a.html.
[57] Iris Cong, Soonwon Choi, and Mikhail D. Lukin. Quantum convolutional neural networks.
Nature Physics, 15(12):1273–1278, August 2019. URL http://doi.org/10.1038/
s41567-019-0648-8.
[58] Arthur G. Rattew and Patrick Rebentrost. Non-linear transformations of quantum amplitudes:
Exponential improvement, generalization, and applications, 2023. URL https://arxiv.
org/abs/2309.09839.
[59] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. Model compression and acceleration for deep
neural networks: The principles, progress, and challenges.IEEE Signal Processing Magazine,
35(1):126–136, 2018. URLhttps://doi.org/10.1109/MSP.2017.2765695.
[60] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. InProcedings of the
British Machine Vision Conference 2016, BMVC 2016, pages 87.1–87.12. British Machine
Vision Association, 2016. URLhttps://doi.org/10.5244/C.30.87.
14

[[PAGE 15]]
[61] Samuel Jaques and Arthur G. Rattew. QRAM: A survey and critique, 2023. URL https:
//arxiv.org/abs/2305.10310.
[62] Alexander M. Dalzell, András Gilyén, Connor T. Hann, Sam McArdle, Grant Salton, Quynh T.
Nguyen, Aleksander Kubica, and Fernando G. S. L. Brandão. A distillation-teleportation pro-
tocol for fault-tolerant QRAM, 2025. URLhttps://arxiv.org/abs/2505.20265.
[63] Tsung-Yu Lin, Aruni RoyChowdhury, and Subhransu Maji. Bilinear CNN models for fine-
grained visual recognition. In2015 IEEE International Conference on Computer Vision
(ICCV), pages 1449–1457. IEEE, December 2015. URL https://doi.org/10.1109/
ICCV.2015.170.
[64] Martin Plesch and ˇCaslav Brukner. Quantum-state preparation with universal gate decomposi-
tions.Phys. Rev. A, 83:032302, March 2011. URL https://link.aps.org/doi/10.
1103/PhysRevA.83.032302.
[65] Yudong Cao, Anargyros Papageorgiou, Iasonas Petras, Joseph Traub, and Sabre Kais. Quan-
tum algorithm and circuit design solving the Poisson equation.New Journal of Physics,
15(1):013021, January 2013. ISSN 1367-2630. URL https://doi.org/10.1088/
1367-2630/15/1/013021.
[66] Andrew M Childs, Jin-Peng Liu, and Aaron Ostrander. High-precision quantum algorithms
for partial differential equations.Quantum, 5:574, 2021. URL https://doi.org/10.
22331/q-2021-11-10-574.
[67] Dominic W. Berry and Pedro C. S. Costa. Quantum algorithm for time-dependent differential
equations using Dyson series.Quantum, 8:1369, June 2024. ISSN 2521-327X. URL
https://doi.org/10.22331/q-2024-06-13-1369.
[68] David Jennings, Matteo Lostaglio, Robert B Lowrie, Sam Pallister, and Andrew T Sornborger.
The cost of solving linear differential equations on a quantum computer: fast-forwarding to
explicit resource counts.Quantum, 8:1553, 2024. URL https://doi.org/10.22331/
q-2024-12-10-1553.
[69] Dong An, Akwum Onwunta, and Gengzhi Yang. Fast-forwarding quantum algorithms for
linear dissipative differential equations, 2024. URL https://arxiv.org/abs/2410.
13189.
[70] Zhong-Xia Shang, Naixu Guo, Dong An, and Qi Zhao. Designing a nearly optimal quantum
algorithm for linear differential equations via Lindbladians.Phys. Rev. Lett., 135:120604, Sep
2025. URLhttps://link.aps.org/doi/10.1103/cvl9-97qg.
[71] Jin-Peng Liu, Herman Øie Kolden, Hari K Krovi, Nuno F Loureiro, Konstantina Trivisa,
and Andrew M Childs. Efficient quantum algorithm for dissipative nonlinear differential
equations.Proceedings of the National Academy of Sciences, 118(35):e2026805118, 2021.
URLhttps://www.pnas.org/doi/abs/10.1073/pnas.2026805118.
[72] Jin-Peng Liu, Dong An, Di Fang, Jiasu Wang, Guang Hao Low, and Stephen Jordan. Ef-
ficient quantum algorithm for nonlinear reaction-diffusion equations and energy estima-
tion.Communications in Mathematical Physics, 404(2):963–1020, 2023. URL https:
//link.springer.com/article/10.1007/s00220-023-04857-9.
[73] Hari Krovi. Improved quantum algorithms for linear and nonlinear differential equations.Quan-
tum, 7:913, February 2023. ISSN 2521-327X. URL https://doi.org/10.22331/
q-2023-02-02-913.
[74] Pedro C. S. Costa, Philipp Schleich, Mauro E. S. Morales, and Dominic W. Berry. Further
improving quantum algorithms for nonlinear differential equations via higher-order methods
and rescaling.npj Quantum Information, 11(1), August 2025. ISSN 2056-6387. URL
https://doi.org/10.1038/s41534-025-01084-z.
[75] Hsuan-Cheng Wu, Jingyao Wang, and Xiantao Li. Quantum algorithms for nonlinear dynamics:
Revisiting Carleman linearization with no dissipative conditions.SIAM J. Sci. Comput., 47(2):
A943–A970, 2025. URLhttps://doi.org/10.1137/24M1665799.
15

[[PAGE 16]]
[76] Kosuke Mitarai, Masahiro Kitagawa, and Keisuke Fujii. Quantum analog-digital conversion.
Phys. Rev. A, 99:012301, Jan 2019. URL https://link.aps.org/doi/10.1103/
PhysRevA.99.012301.
[77] V . Giovannetti, S. Lloyd, and L. Maccone. Quantum random access memory.Physical Re-
view Letters, 100(16):160501, 2008. URL https://link.aps.org/doi/10.1103/
PhysRevLett.100.160501.
[78] Connor T. Hann, Gideon Lee, S.M. Girvin, and Liang Jiang. Resilience of quantum random
access memory to generic noise.PRX Quantum, 2:020311, Apr 2021. URL https://link.
aps.org/doi/10.1103/PRXQuantum.2.020311.
[79] Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone. Architectures for a quantum random
access memory.Phys. Rev. A, 78:052310, Nov 2008. URL https://link.aps.org/
doi/10.1103/PhysRevA.78.052310.
[80] Anupam Prakash.Quantum algorithms for linear algebra and machine learning. PhD thesis,
University of California, Berkeley, 2014. URL https://digicoll.lib.berkeley.
edu/record/136504.
[81] Iordanis Kerenidis and Anupam Prakash. Quantum recommendation systems. In8th Innova-
tions in Theoretical Computer Science Conference (ITCS 2017), volume 67 ofLeibniz Inter-
national Proceedings in Informatics (LIPIcs), pages 49:1–49:21. Schloss Dagstuhl–Leibniz-
Zentrum fuer Informatik, 2017. URL https://doi.org/10.4230/LIPIcs.ITCS.
2017.49.
[82] Lov Grover and Terry Rudolph. Creating superpositions that correspond to efficiently inte-
grable probability distributions, 2002. URL https://arxiv.org/abs/quant-ph/
0208112.
[83] Daan Camps and Roel Van Beeumen. Approximate quantum circuit synthesis using block
encodings.Phys. Rev. A, 102:052411, Nov 2020. URL https://link.aps.org/doi/
10.1103/PhysRevA.102.052411.
[84] Shantanav Chakraborty, Aditya Morolia, and Anurudh Peduri. Quantum regularized least
squares.Quantum, 7:988, April 2023. ISSN 2521-327X. URL https://doi.org/10.
22331/q-2023-04-27-988.
[85] Kaito Wada, Naoki Yamamoto, and Nobuyuki Yoshioka. Heisenberg-limited adaptive gradient
estimation for multiple observables.PRX Quantum, 6:020308, Apr 2025. URL https:
//link.aps.org/doi/10.1103/PRXQuantum.6.020308.
[86] Guang Hao Low and Isaac L. Chuang. Hamiltonian simulation by uniform spectral amplifica-
tion, 2017. URLhttps://arxiv.org/abs/1707.05391.
[87] Andrew M. Childs and Nathan Wiebe. Hamiltonian simulation using linear combinations of
unitary operations.Quantum Information and Computation, 12(11 & 12):901–924, November
2012. ISSN 1533-7146. URL https://dl.acm.org/doi/abs/10.5555/2481569.
2481570.
[88] Mehdi Saeedi and Massoud Pedram. Linear-depth quantum circuits for n-qubit toffoli gates
with no ancilla.Phys. Rev. A, 87:062318, Jun 2013. URL https://link.aps.org/
doi/10.1103/PhysRevA.87.062318.
[89] Arthur G. Rattew and Bálint Koczor. Preparing arbitrary continuous functions in quantum
registers with logarithmic complexity, 2022. URL https://arxiv.org/abs/2205.
00519.
[90] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-
son, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Py-
Torch: An imperative style, high-performance deep learning library. In H. Wallach,
16

[[PAGE 17]]
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Ad-
vances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/
hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html.
[91] Danial Motlagh and Nathan Wiebe. Generalized quantum signal processing.PRX Quantum,
5:020368, Jun 2024. doi: 10.1103/PRXQuantum.5.020368. URL https://link.aps.
org/doi/10.1103/PRXQuantum.5.020368.
[92] Daan Camps, Lin Lin, Roel Van Beeumen, and Chao Yang. Explicit quantum circuits for
block encodings of certain sparse matrices.SIAM J. Matrix Anal. Appl., 45(1):801–827, 2024.
URLhttps://doi.org/10.1137/22M1484298.
[93] Thomas G. Draper. Addition on a quantum computer, 2000. URL https://arxiv.org/
abs/quant-ph/0008033.
[94] Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional
layers. InInternational Conference on Learning Representations, 2019. URL https:
//openreview.net/forum?id=rJevYoA9Fm.
[95] D. Coppersmith. An approximate Fourier transform useful in quantum factoring, 2002. URL
https://arxiv.org/abs/quant-ph/0201067.
[96] Naixu Guo, Kosuke Mitarai, and Keisuke Fujii. Nonlinear transformation of complex ampli-
tudes via quantum singular value transformation.Phys. Rev. Res., 6:043227, December 2024.
URLhttps://link.aps.org/doi/10.1103/PhysRevResearch.6.043227.
[97] Marcus Cramer, Martin B. Plenio, Steven T. Flammia, Rolando Somma, David Gross,
Stephen D. Bartlett, Olivier Landon-Cardinal, David Poulin, and Yi-Kai Liu. Efficient quantum
state tomography.Nature Communications, 1(1), December 2010. ISSN 2041-1723. URL
https://doi.org/10.1038/ncomms1147.
[98] Joran van Apeldoorn, Arjan Cornelissen, András Gilyén, and Giacomo Nannicini. Quantum
tomography using state-preparation unitaries. InProceedings of the 2023 Annual ACM-SIAM
Symposium on Discrete Algorithms (SODA), page 1265–1318. Society for Industrial and
Applied Mathematics, January 2023. ISBN 9781611977554. URL https://doi.org/
10.1137/1.9781611977554.ch47.
[99] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normal-
ization for generative adversarial networks, 2018. URL https://openreview.net/
forum?id=B1QRgziT-.
[100] Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generaliz-
ability of deep learning, 2017. URLhttps://arxiv.org/abs/1705.10941.
[101] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisa-
tion of neural networks by enforcing Lipschitz continuity.Machine Learning, 110(2):
393–416, December 2020. ISSN 1573-0565. URL https://doi.org/10.1007/
s10994-020-05929-w.
[102] Connor T Hann.Practicality of quantum random access memory. PhD thesis,
Yale University, 2021. URL https://elischolar.library.yale.edu/gsas_
dissertations/346.
[103] Aaron Carroll and Gernot Heiser. An analysis of power consumption in a smartphone. In
Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference,
USENIXATC’10, page 21, USA, 2010. USENIX Association. URL https://dl.acm.
org/doi/10.5555/1855840.1855861.
[104] Aqeel Mahesri and Vibhore Vardhan. Power consumption breakdown on a modern laptop.
InPower-Aware Computer Systems, page 165–180. Springer Berlin Heidelberg, 2005. ISBN
9783540314851. URLhttps://doi.org/10.1007/11574859_12.
17

[[PAGE 18]]
[105] Kazi Main Uddin Ahmed, Math HJ Bollen, and Manuel Alvarez. A review of data centers
energy consumption and reliability modeling.IEEE access, 9:152536–152563, 2021. URL
https://doi.org/10.1109/ACCESS.2021.3125092.
[106] Xiaobo Fan, Wolf-Dietrich Weber, and Luiz Andre Barroso. Power provisioning for a
warehouse-sized computer.SIGARCH Comput. Archit. News, 35(2):13–23, June 2007. ISSN
0163-5964. URLhttps://doi.org/10.1145/1273440.1250665.
[107] Ryan Babbush, Jarrod R. McClean, Michael Newman, Craig Gidney, Sergio Boixo, and
Hartmut Neven. Focus beyond quadratic speedups for error-corrected quantum advantage.
PRX Quantum, 2:010103, Mar 2021. URL https://link.aps.org/doi/10.1103/
PRXQuantum.2.010103.
[108] Fang-Yu Hong, Yang Xiang, Zhi-Yan Zhu, Li-zhen Jiang, and Liang-neng Wu. Robust quantum
random access memory.Phys. Rev. A, 86:010306, Jul 2012. URL https://link.aps.
org/doi/10.1103/PhysRevA.86.010306.
[109] Ewin Tang. A quantum-inspired classical algorithm for recommendation systems. InPro-
ceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019,
page 217–228, New York, NY , USA, 2019. Association for Computing Machinery. ISBN
9781450367059. URLhttps://doi.org/10.1145/3313276.3316310.
[110] Nai-Hui Chia, András Pal Gilyén, Tongyang Li, Han-Hsuan Lin, Ewin Tang, and Chunhao
Wang. Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing
quantum machine learning.Journal of the ACM, 69(5):1–72, October 2022. ISSN 1557-735X.
URLhttps://doi.org/10.1145/3549524.
[111] Sevag Gharibian and François Le Gall. Dequantizing the quantum singular value transforma-
tion: Hardness and applications to quantum chemistry and the quantum PCP conjecture.SIAM
Journal on Computing, 52(4):1009–1038, 2023. URL https://doi.org/10.1137/
22M1513721.
[112] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization
in overparameterized neural networks, going beyond two layers. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Ad-
vances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/
hash/62dad6e273d32235ae02b7d321578ee8-Abstract.html.
[113] Hela Mhiri, Ricard Puig, Sacha Lerch, Manuel S. Rudolph, Thiparat Chotibut, Supanut
Thanasilp, and Zoë Holmes. A unifying account of warm start guarantees for patches of
quantum landscapes, 2025. URLhttps://arxiv.org/abs/2502.07889.
[114] Sushant Sachdeva and Nisheeth K Vishnoi. Faster algorithms via approximation theory.
Foundations and Trends® in Theoretical Computer Science, 9(2):125–210, 2014. ISSN
1551-3068. URLhttps://doi.org/10.1561/0400000065.
TECHNICALAPPENDICES ANDSUPPLEMENTARYMATERIAL
In Appendix A we present a summary of Quantum Random Access Memory (QRAM), which we
subsequently use. In Appendix B we present a number of existing techniques which we require
to manipulate vectors and matrices with quantum computers, and then use them to develop a
number of new useful results for quantum matrix-vector arithmetic. In Appendix C, we use the
techniques developed in Appendix B to construct quantum-implementations of key architectural
blocks. In Appendix D, we discuss the feasibility of QRAM. In Appendix E we use the architectural
blocks obtained in Appendix C to derive end-to-end complexities for a number of architectures under
different QRAM assumptions.
18

[[PAGE 19]]
A QUANTUMRANDOMACCESSMEMORY(QRAM)
Quantum Random Access Memory [ 77] is a widely assumed mechanism in the quantum computing
literature for accessing data in a quantum computer. In this paper, we make a range of QRAM
assumptions under different regimes of assumed feasibility. With the aim of enabling practical
end-to-end speed-ups, it is important to explicitly state the different assumptions and consider the
feasibility of each of these regimes.
In this section, we will formally define QRAM, and state the assumed complexities. In Appendix D,
we dive into a deeper discussion of the feasibility of our various QRAM assumptions, with the aim
of providing a clear understanding of what sorts of end-to-end speed-ups our results can offer in
practice.
Definition A.1(QRAM for Classical Data).Let N= 2nandD= 2d. Let|i⟩nbe any n-qubit
standard basis vector, and let xi∈[D] . Then, a QRAM with O(dNlogN) total qubits can implement
the mapping,
U|i⟩ n|0⟩d=|i⟩ n|xi⟩d (A.1)
withO(dlogN)circuit depth.
As mentioned in a number of sources, e.g., Hann et al. [78], Giovannetti et al. [79] anNqubit QRAM
can be implemented with O(logN) depth complexity. Consequently, performing a sequence of d
of these (to implement each of the d-bits in each memory register), a circuit depth complexity of
O(dlogN)trivially follows.
Definition A.2(QRAM for Quantum Data [ 80,81,41]).Let N= 2n,M= 2m. Let|i⟩nbe any
n-qubit standard basis vector. Allow |ψi⟩mto be an arbitrary m-qubit normalized quantum states.
Then, a QRAM with ˜O(MN) total qubits, and ˜O(MN) classical pre-processing to construct the
data-structure, can implement the mapping,
U|i⟩ n|0⟩m=|i⟩ n|ψi⟩m (A.2)
withO(log2(NM))circuit depth.
Importantly, as per Prakash [80], Kerenidis and Prakash [81], Kerenidis et al. [41] QRAM for quantum
data can be implemented by a circuit (based on Grover and Rudolph [82]) with depth and width
O(polylog(MN)) with access to a QRAM data structure (as per Definition A.1) containing all the
entries of each state in the quantum data (along with O(logM) copies for each of the sets of partial
norms). Thus, if QRAM for classical data is feasible (as discussed in Appendix D), QRAM for
quantum data is as well (with pre-processing to construct the appropriate data-structures).
In this work, we will use QRAM to describe QRAM for both quantum and classical data, and will
make the distinction clear when it is relevant.
B QUANTUMMATRIX-VECTORARITHMETIC
In this section, we formally derive a number of tools for quantum matrix-vector arithmetic.
Lemma B.1(Product of block encodings [ 36]).If Uis an (α, a, δ) -block-encoding of an s-qubit
operator A, and Vis an(β, b, ϵ) -block-encoding of an s-qubit operator Bthen(Ib⊗U)(I a⊗V) is
an(αβ, a+b, αϵ+βδ)-block-encoding ofAB.
In Lemma B.1, following the convention of Gilyén et al. [36], it is assumed that Uacts trivially on
the ancillas of V, andVacts trivially on the ancillas of U– this notation is not used anywhere else in
this paper. I.e., the tensor products in (Ib⊗U)(I a⊗V) are not read in the usual sense in this lemma.
We now present a standard result (see Lemma 1 of Camps and Van Beeumen [83] or Lemma
21 of Chakraborty et al. [84]), and we include the proof for completeness, as it is the basis of a
subsequent proof Lemma 3. In particular, our derivation closely follows that of Lemma 1 of Camps
and Van Beeumen [83].
Lemma B.2(Tensor Product of Block-Encoded Operators).Given a unitary UAwhich is an (α, a, ϵ 0)-
block-encoding for n-qubit operator AwithO(TA)circuit complexity, and a unitary UBwhich is a
19

[[PAGE 20]]
(β, b, ϵ 1)-block-encoding for m-qubit operator BwithO(TB)circuit complexity, we can obtain an
(αβ, a+b, ϵ 0β+ϵ 1α+ϵ 0ϵ1)-block-encoding for A⊗B withO(max(T A, TB) + max(n, b)) circuit
complexity.
Proof. The main idea is that UA⊗UBalmost directly implements a block-encoding of A⊗B , but
the ancillas and the main computation registers are in the wrong order. To correct this, we need to
swap the ancilla register ofU Bwith the main register ofU A.
Consequently, define the operator Πsuch that it swaps the n-qubit register with the b-qubit register
(and leaves the other registers unchanged), so that all the ancilla registers precede the main registers.
Ifn≥b ,Πcan be implemented by a sequence of O(n/b) swaps, with each swap swapping
O(b) qubits in parallel. If n < b , then it can be implemented with O(b/n) swaps. Thus, Πhas
a circuit depth bounded by O(max(n/b, b/n))∈O(max(n, b)) . Then, Π(|0⟩ a+b⊗In+m) =
(|0⟩a⊗In)⊗(|0⟩ b⊗Im), and(⟨0| a+b⊗In+m)Π†= (⟨0| a⊗In)⊗(⟨0| b⊗Im).
Following Camps and Van Beeumen [83], define ˜A:= (⟨0| a⊗In)UA(|0⟩a⊗In), and ˜B:=
(⟨0|b⊗Im)UB(|0⟩b⊗Im). LetEA:=A−α ˜A, and let EB:=B−β ˜B. Define V:= Π†(UA⊗UB)Π.
Then, A⊗B= (α ˜A+E A)⊗(β ˜B⊗E B), and (⟨0|a+b⊗In+m)V(|0⟩ a+b⊗In+m) =˜A⊗ ˜B, so
∥A⊗B−αβ(⟨0| a+b⊗In+m)V(|0⟩ a+b⊗In+m)∥2(B.1)
=(α˜A+E A)⊗(β ˜B⊗E B)−αβ ˜A⊗ ˜B
2(B.2)
≤ϵ0β+ϵ 1α+ϵ 0ϵ1.(B.3)
We now present a result from the literature allowing a block-encoding to have all of its singular values
scaled by a constant value. We present the result nearly verbatim from Lemma 5 of Wada et al. [85]
(with trivial modifications to make it easier to invoke in our context), which presents the results of
Low and Chuang [86], Gilyén et al. [36] cleanly in the language of block-encodings.
Lemma B.3(Uniform Singular Value Amplification [ 85,86,36]).Let ϵ, δ∈(0,1/2) , and let
γ >1 . Let UAbe an (1, a,0) -block-encoding of the n-qubit operator AwithO(T) circuit depth.
Suppose ∥A∥2≤(1−δ)/γ . Then, we can obtain a quantum circuit Vwhich is a (1, a+ 1, ϵ) -block-
encoding for γAwithO(γ
δlog(γ/ϵ)(T+a)) circuit depth, and with O(poly(γ
δlog(γ/ϵ))) classical
computation to determine the QSVT rotation angles.
Proof. This is taken directly from Wada et al. [85], Low and Chuang [86], Gilyén et al. [36], simply
noting that an a-controlled Xgate can be implemented by a sequence of O(a) single and two-qubit
gates.
We now present a simple result which is just a special case of uniform singular value amplification [ 85,
86,36] in the case where all the singular values of an encoded operator are either 0or1/2. This is
done following the ideas of oblivious amplitude amplification (see Gilyén et al. [36]).
Lemma B.4(1
2Oblivious Amplitude Amplification).We are given a matrix A∈CN×N, with
singular values either 1or0. Assume we have access to UAa(2, a,0) -BE of AwithO(T) circuit
depth. One can construct (1, a+ 1,0) -BE of AwithO(T) circuit depth, and with 3 calls to a
controlled-Ucircuit.
Proof. Note that T3(x) = 4x3−3x satisfies the condition that |T3(x)| ≤1 forx∈[−1,1] and
T3(1
2) =−1 . Therefore, one can achieve the task by implementing the function −T3(x)via QSVT
and the block encoding. The first kind of the Chebyshev polynomial can be directly achieved without
any classical processing to determine angle rotations, so one can construct the block encoding with
no error.
For completeness, we now re-derive an existing result on the linear combination of block-encoded
matrices, directly following Gilyén et al. [36] (which presents the result of Childs and Wiebe [87] in
the context of block-encodings).
20

[[PAGE 21]]
Lemma B.5(Linear Combination of Block-Encodings [ 87,36]).Suppose we are given a set of
D= 2dunitaries {Ui}isuch that each Uiis an (α, a, ϵ) -block-encoding for nqubit operator Ai,
and each Uihas a total of O(T 0)single and two qubit gates. Define the vector b∈CDsuch that
b= ( b0b1. . . b D−1)T. Define |b⟩d=PD
j=0p
bj|j⟩dandβ:=∥|b⟩ d∥2
2=∥b∥1. We are given
thed-qubit unitary Ub, with O(T 1)single and two qubit gates, such that Ub|0⟩d=|b⟩ d/∥|b⟩ d∥2.
Define A:=PD−1
j=0bjAj. Then, we can obtain a unitary VwithO(dDT 0+T1)circuit depth which
is an(αβ, a+d, αβϵ)-block-encoding forA.
Proof. For each j∈[D] , let˜Aj:= (⟨0| a⊗In)Uj(|0⟩a⊗In), and let Ej:=A j−α˜Aj. Define
S:=PD−1
j=0|j⟩⟨j| d⊗Uj. Note that Scan be implemented by a sequence of Dmulti-controlled Uj
operators. Note that by using Saeedi and Pedram [88], adcontrolled gate targeting 1or2qubits can
be decomposed into a sequence of O(d) single and two qubit gates. Consequently, each d-controlled
UjhasO(dT 0)circuit depth in terms of single and two qubit gates. Thus, Sconsists of a total of
O(dDT 0)single and two qubit gates. Then, defineV:= (U†
b⊗Ia+n)S(U b⊗Ia+n).
Noting that (⟨0|d⊗Ia+n)V(|0⟩ d⊗Ia+n) =1
βPD−1
j=0bjUj. Using the fact that |0⟩a+d⊗In=
(|0⟩d⊗Ia+n)(|0⟩ a⊗In), we then obtain
(⟨0|a+d⊗In)V(|0⟩ a+d⊗In) =1
β(⟨0|a⊗In)(D−1X
j=0bjUj)(|0⟩ a⊗In) =1
βD−1X
j=0bj˜Aj.(B.4)
Consequently,
∥A−αβ(⟨0| a+d⊗In)V(|0⟩ a+d⊗In)∥2=D−1X
j=0bj(α˜Aj+Ej)−D−1X
j=0αbj˜Aj
2(B.5)
=D−1X
j=0bjαEj
2≤αD−1X
j=0|bj|∥Ej∥2(B.6)
≤αβϵ.(B.7)
Thus,Vgives a(αβ, a, αβϵ)-block-encoding forA, and hasO(dDT 0+T1)circuit depth.
The following is a standard result which has been used in various contexts, and is included for
completeness.
Lemma B.6(Block Encoding of Rank 1 Projector of Basis Vectors).Let n∈N ≥0, and let N= 2n.
Define i∈[N] andj∈[N] . Then, we can get a unitary Uwhich is a (1,2,0) -block-encoding of the
nqubit operator|i⟩⟨j|. Moreover,UhasO(n)circuit depth.
Proof. Following Jaques and Rattew [61], a(1,2,0) block-encoding of the matrix |0⟩⟨0| , call it
V, can be obtained with O(n) circuit complexity. This follows by constructing a (1,0,0) block-
encoding of the Grover reflection operator, I−2|0⟩⟨0| , and taking a linear combination with I
via the sum of block-encoding result of Gilyén et al. [36]. The circuit complexity is dominated
by reflection operator, which can be implemented by applying a n−1 controlled XZX gate
on the most significant qubit, controlled on the 0state of the other n−1 qubits. Using Saeedi
and Pedram [88] this can be decomposed into a sequence of O(n) two-qubit gates. Decompose
iandjinto bits as, i=i 0i1. . . in−1, and j=j 0j1. . . jn−1. We now define two operators,
Mi:=Xi0⊗Xi1⊗. . .⊗Xin−1andMj:=Xj0⊗Xj1⊗. . .⊗Xjn−1. Clearly, Mi|0⟩⟨0|M j=|i⟩⟨j| .
Then, since (I2⊗M i)V(I 2⊗M j) =
|i⟩⟨j| ·
· ·
. Thus, (I2⊗M i)V(I 2⊗M j)is a(1,2,0) block-
encoding for|i⟩⟨j|.
We now present a simple result which helps intuitively visualize VEs as encoding vectors in a
subspace.
21

[[PAGE 22]]
Lemma B.7(Intuitive Picture of VE as a Vector Subspace Encoding).Let Uψbe an (α, a, ϵ) -VE
for|ψ⟩n. Define |Eψ⟩n:=|ψ⟩ n−α(⟨0| a⊗In)Uψ|0⟩a+n. Define the a-qubit operator pa
j:=|j⟩⟨j| .
Then,
Uψ|0⟩a+n=|0⟩a|ψ⟩n− |0⟩ a|Eψ⟩n
α+2a−1X
j=1(pa
j⊗In)Uψ|0⟩a+n= |ψ⟩n−|Eψ⟩n
α...!
.(B.8)
Proof.|E ψ⟩n=|ψ⟩ n−α(⟨0| a⊗In)Uψ|0⟩a+n implies that |0⟩a|Eψ⟩n=|0⟩ a|ψ⟩n−α(pj
0⊗
In)Uψ|0⟩a+n. The result follows trivially by algebraic maniuplation of Uψ|0⟩a+n= (P2a−1
j=0pa
j⊗
In)Uψ|0⟩a+n.
Intuitively, in the absence of error, the first 2nentries of Uψ|0⟩a+nwill contain the sub-normalized
vector|ψ⟩ n/α.
We now state the following result from Rattew and Rebentrost [58] nearly verbatim, slightly improving
the complexity. The following result is a tool essentially implementing ℓ2layer normalization, follows
directly from oblivious amplitude amplification (see e.g., Gilyén et al. [36]), and is taken nearly
verbatim from Rattew and Rebentrost [58].
Lemma B.8(Vector Normalization, Lemma 18 of Rattew and Rebentrost [58]).Let ϵ0∈[0,1/2] ,
α≥1 ,a∈N ,ϵ1>0. Letα′be a known bound such that α′≥α. Given a unitary Uψ, a(α, a, ϵ 0)-
VE for the ℓ2-normalized quantum state |ψ⟩nwith circuit complexity O(Tψ), we can construct a
(1, a+ 4,2(ϵ 0+ϵ1))-VE for |ψ⟩nwith circuit complexity O((T ψ+a+n)α′log(1/ϵ 1))and with
O(α′log(1/ϵ 1))queries to aU ψandU†
ψcircuit.
This implements vector normalization by boosting the scaling factor so the norm of the encoded
vector is1, and all the padding entries are0(up to logarithmic error).
Proof. Define |ϕ⟩n:= (⟨0| a⊗In)Uψ|0⟩a+n,Nϕ:=∥|ϕ⟩ n∥2,|Φ⟩n:=|ϕ⟩ n/Nϕ. Then, Uψis
equivalently a (Nϕ, a,0) -VE for |ϕ⟩n/Nϕ. Using Lemma B.6, we can get U0a(1,2,0) -block-
encoding of the n+a qubit projector |0⟩⟨0| withO(n+a) circuit depth. Then, V= (I 2⊗Uψ)U0is
a(1,2,0) -block-encoding for Uψ|0⟩⟨0| , with O(Tψ+a+n) circuit complexity. Noting that (⟨0|2⊗
Ia+n)V(|0⟩ 2⊗Ia+n) =U ψ|0⟩⟨0| , then (⟨0|2+a⊗In)V(|0⟩ 2+a⊗In) = (⟨0| a⊗In)Uψ|0⟩⟨0|(|0⟩ a⊗
In) =|ϕ⟩⟨0| a, so
∥|ϕ⟩⟨0| a− ⟨0| 2+a⊗In)V(|0⟩ 2+a⊗In)∥2= 0.(B.9)
Thus, we have a (1, a+ 2,0) -block-encoding of |ϕ⟩⟨0| a=N ϕ|Φ⟩⟨0| . This object has singular value
Nϕ. Thus, we want to apply a polynomial approximation to this block-encoding, such that the error
of the polynomial approximation is at most ϵ1on the interval [Nϕ,1]. From Corollary 6 of Low and
Chuang [86], we know that there exists an odd polynomial Pk(x)with degree k∈O(1
τlog(1/ϵ 1))
such that
max
x∈[−1,−τ
2]∪[τ/2,1]|Pk(x)−sign(x)| ≤ϵ 1 (B.10)
andmax x∈[−1,1] |Pk(x)| ≤1 . Since Nϕ≥1
2α≥1
2α′, we can set τ=1
2α′, guaranteeing that
P(N ϕ)≥1−ϵ 1. Consequently, we can invoke quantum singular value transformation (QSVT)
[36] with Pk, yielding Vfa(1, a+ 4, ϵ 1)-block-encoding for P(N ϕ|Φ⟩⟨0|) =c|Φ⟩⟨0| , where
1≥c≥1−ϵ 1. Moreover,V fhasO(1
α′log(1/ϵ 1)(Tψ+a+n))circuit complexity. Noting that
∥|ψ⟩⟨0| −P(N ϕ|Φ⟩⟨0|)∥2=∥|ψ⟩⟨0| − |Φ⟩⟨0|+|Φ⟩⟨0| −c|Φ⟩⟨0|∥2(B.11)
≤ ∥|ψ⟩⟨0| − |Φ⟩⟨0|∥2+∥|Φ⟩⟨0| −c|Φ⟩⟨0|∥2(B.12)
≤ ∥|ψ⟩ n− |Φ⟩ n∥2+ϵ1.(B.13)
Moreover,
∥|ψ⟩ n− |Φ⟩ n∥2≤∥|ψ⟩ n−α|ϕ⟩ n∥2+∥α|ϕ⟩ n−1
Nϕ|ϕ⟩n∥2 (B.14)
≤ϵ0+1
Nϕ∥αN ϕ|ϕ⟩n− |ϕ⟩ n∥=ϵ 0+|αNϕ−1|
Nϕ∥|ϕ⟩n∥2 (B.15)
≤ϵ0+|αN ϕ−1|.(B.16)
22

[[PAGE 23]]
Moreover, using the reverse triangle inequality with ∥|ψ⟩ n−α|ϕ⟩ n∥2≤ϵ0, we get |1−α∥|ϕ⟩ n∥2|=
|1−αN ϕ| ≤ϵ 1, which implies that 1−ϵ 0≤αN ϕ≤1 +ϵ 0. Consequently, |αNϕ−1| ≤ϵ 0, and so
∥|ψ⟩ n− |Φ⟩ n∥2≤2ϵ 0.(B.17)
Thus,
∥|ψ⟩⟨0| −P(N ϕ|Φ⟩⟨0|)∥2≤2ϵ 0+ϵ1.(B.18)
Moreover, sinceV fis a(1, a+ 4, ϵ 1)-block-encoding forP(N ϕ|Φ⟩⟨0|),
∥P(N ϕ|Φ⟩⟨0|)−(⟨0| a+4⊗In)Vf(|0⟩a+4⊗In)∥2≤ϵ1.(B.19)
Thus,
∥|ψ⟩⟨0| −(⟨0| a+4⊗In)Vf(|0⟩a+4⊗In)∥2≤2(ϵ 0+ϵ1).(B.20)
Sometimes it is necessary to increase the norm of the vector encoded in the subspace of a VE. This
is equivalent to multiplying all of the entries in the encoded vector by a constant with value greater
than or equal to one. The following lemma achieves the opposite: it allows the norm of the encoded
vector to be shrunk by an arbitrarily large amount. This is equivalent to dividing all the entries in the
encoded vector by a constant greater than or equal to one. It is worth noting that the following result
is trivial and can almost certainly be further optimized, e.g., by removing the additional ancillary
qubits added.
Lemma B.9(Vector De-Amplification).Let τ≥1 ,α≥1 ,ϵ≥0 . Given Uψan(α, a, ϵ) -VE for
|ψ⟩n, with circuit complexity O(T) , we can obtain U′
ψan(ατ, a+ 2, ϵ) -VE for |ψ⟩nwith circuit
complexityO(T+a).
Proof. Let|ϕj⟩n:= (⟨0| a⊗In)Uψ|0⟩a+n. Then, note that Uψ|0⟩a+n=P2a−1
j=0|j⟩a⊗ |ϕ j⟩n.
By Definition 3, we know that∥|ψ⟩ n−α|ϕ 0⟩n∥ ≤ϵ.
We introduce two single-qubit ancillas as the most significant bits, and then apply a multiple-controlled
Xgate (with acontrols each activated by the 0state of each of the previous aancilla qubits) targeting
the first newly added ancilla qubit. Using Saeedi and Pedram [88] this can be implemented with O(a)
two-qubit gates. We then apply a controlled R1/τ2(as per Definition B.1) gate targeting the second
new ancilla qubit, controlled on the first new ancilla. This yields the state,
|1⟩1(1
τ|0⟩1+r
1−1
τ2|1⟩1)|0⟩a|ϕ0⟩n+|0⟩ 1|0⟩12a−1X
j=1|j⟩a|ϕj⟩n.(B.21)
We then apply a Xgate to the first ancilla qubit, and we call the 2 +a -qubit unitary containing all
the preceding operations V. Then, U′
ψ:= (V⊗I n)(I2⊗Uψ). Simple analysis thus shows that
(⟨0|2+a⊗In)U′
ψ|0⟩2+a+n =|ϕ 0⟩n/τ. Then,
|ψ⟩n−ατ(⟨0| 2+a⊗In)U′
ψ|0⟩2+a+n
2=∥|ψ⟩ n−α|ϕ 0⟩n∥ ≤ϵ.(B.22)
Definition B.1(Real Rotation Single Qubit Gate).Let 0≤τ≤1 . Then, define the following
single-qubit gate:
Rτ:=√τ−√1−τ√1−τ√τ,
.(B.23)
Proof of Lemma 1(Vector Sum). This result follows using a common techniques, see e.g., LCU [ 87],
or the sum of block-encodings result [ 36]. As per Figure 3, we will augment UψandUϕso that they
both act on c= max(a+b) ancilla qubits. Then, define the n+c qubit states, |˜ψ⟩n+c:=U ψ|0⟩n+c.
We will drop the subscripts on these states for the rest of the proof, as their dimension is clear from the
context. This block-encoding will be obtained with the circuit shown in Figure 3, and so we will now
23

[[PAGE 24]]
Figure 3:Circuit for addition of VE encoded vectors.Given two unitary matrices, Uψwhich is
a(α, a, ϵ 0)-VE for the n-qubit state |ψ⟩, and Uϕwhich is a (β, b, ϵ 1)-VE for the n-qubit state |ϕ⟩,
define c:= max(a, b) . We define ˜Uψby appropriately tensoring UψwithIc−aand we define ˜Uϕby
appropriately tensoring UϕwithIc−b, such that ˜Uψand˜Uϕboth act on n+c qubits. Then, the given
circuit yields a VE of the sum of the encoded vectors, as shown in Lemma 1.
analyze the action of that circuit. First, we start with the state |0⟩1+n+c , which we will write as |0⟩|0⟩ ,
where the first register has one qubit, and the second register has the remaining n+c qubits. We then
apply Rτ(as defined in Definition B.1) to the first qubit, yielding the state (√τ|0⟩+√1−τ|1⟩)|0⟩ .
Next, we apply the controlled UψandUϕgates, yielding,√τ|0⟩| ˜ψ⟩+√1−τ|1⟩| ˜ϕ⟩. Next, we
apply R†
τ=√τ√1−τ
−√1−τ√τ
on the first qubit, yielding the output of the new VE, V|0⟩=
|0⟩(τ| ˜ψ⟩+ (1−τ)| ˜ϕ⟩) +p
τ(1−τ)|1⟩(| ˜ϕ⟩ − | ˜ψ⟩). Define |Eψ⟩:=|ψ⟩ −α(⟨0|⊗(c)⊗In)|˜ψ⟩and
note that ∥|Eψ⟩∥2≤ϵ0. Similarly define |Eϕ⟩, and note that ∥|Eϕ⟩∥2≤ϵ1. As a result, we can
determine the properties of this VE by bounding the following,
|Γ⟩ −1
N(⟨0|⊗(1+c)⊗In)V|0⟩ 1+c+n
2(B.24)
=|Γ⟩ −1
N(⟨0|⊗c⊗In)(τ|˜ψ⟩+ (1−τ)| ˜ϕ⟩)
2(B.25)
=|Γ⟩ −1
Nτ
α(|ψ⟩ − |E ψ⟩) +1−τ
β(|ϕ⟩ − |E ϕ⟩)
2(B.26)
=1
Nτ
α|Eψ⟩+1−τ
β|Eϕ⟩
2≤1
Nτϵ0
α+(1−τ)ϵ 1
β
(B.27)
≤1
Nϵ0
α+ϵ1
β
≤ϵ0+ϵ1
N.(B.28)
where the final inequality comes from the definition of a VE imposing that α≥1 andβ≥1 . Thus,
the unitary circuitVis a(N−1,1 +a+b,N−1(ϵ0+ϵ1))-VE for| Γ⟩.
Proof of Lemma 2 (Matrix Vector Product). We now require a result allowing for matrix-vector
products with our vector-encodings. This result is essentially a special case of the product of the
standard product of block-encodings result (Lemma 53 of Gilyén et al. [36]). As a result, the following
proof closely follows that in Gilyén et al. [36].
It is important to note that in this lemma, following the convention of Gilyén et al. [36], it is assumed
thatUAacts trivially on the ancillas of Uψ, andUψacts trivially on the ancillas of UA– this notation
is not used anywhere else in this paper. I.e., the tensor products in (Ib⊗UA)(Ia⊗Uψ)are not read
in the usual sense in this lemma. LetN:=∥A|ψ⟩ n∥2. We wish to upper-bound,
ξ:=A|ψ⟩ n
N−αβ
N(⟨0|a+b⊗In)(Ib⊗UA)(Ia⊗Uψ)|0⟩a+b+n
2(B.29)
=1
N∥A|ψ⟩ n−αβ(⟨0| a+b⊗In)(Ib⊗UA)(Ia⊗Uψ)(|0⟩ a+b⊗In)|0⟩n∥2(B.30)
Then, directly from the proof of Lemma 53 in Gilyén et al. [36],
ξ=1
N∥A|ψ⟩ n−αβ[(⟨0| a⊗In)UA(|0⟩a⊗In)] [(⟨0| b⊗In)Uψ(|0⟩b⊗In)]|0⟩ n∥2(B.31)
24

[[PAGE 25]]
Let˜A:=α(⟨0| a⊗In)UA(|0⟩a⊗In)and let| ˜ψ⟩:=β(⟨0| b⊗In)Uψ(|0⟩b+n). Then,
ξ=1
NA|ψ⟩ n−˜A|˜ψ⟩n
2=1
NA|ψ⟩ n−˜A|ψ⟩ n+˜A|ψ⟩ n−˜A|˜ψ⟩n
2(B.32)
≤1
NA− ˜A
2+˜A
2|ψ⟩n− |˜ψ⟩n
2
(B.33)
Noting that˜A
2≤α, we then get
ξ≤(ϵ 0+αϵ 1)/N.(B.34)
Consequently,(I b⊗UA)(Ia⊗Uψ)gives a(αβ/N, a+b,(ϵ 0+αϵ 1)/N)-VE forA|ψ⟩ n/N.
In the following lemma we derive a technical result handling the case where you have a vector
encoding for some vector |ψ⟩, and another vector of interest |ϕ⟩is sub-encoded as |ψ⟩=
|ϕ⟩/β
·
.
Our result also handles the case where each vector is imperfectly encoded (i.e., encoded with error).
Lemma B.10(Vector Sub-Encodings).Let m, n be integers such that m > n . Let Uψbe an
(α, a, ϵ) -VE for |ψ⟩m, and let |ψ⟩m≈Vϕ|0⟩m(precisely, ∥|ψ⟩ m−Vϕ|0⟩m∥2≤γ), where Vϕis a
(β, m−n, δ)-VE for|ϕ⟩ n. Then,U ψis an(αβ, a+m−n, δ+β(ϵ+γ))-VE for|ϕ⟩ n.
Proof. Letb=m−n . First, define |Eψ⟩m:=|ψ⟩ m−α(⟨0| a⊗Im)Uψ|0⟩a+m, and|Eϕ⟩n:=
|ϕ⟩n−α(⟨0| b⊗In)Uψ|0⟩b+n. By Definition 3, ∥|Eψ⟩m∥2≤ϵand∥|Eϕ⟩n∥2≤δ. Let|Ev⟩m:=
|ψ⟩m−Vϕ|0⟩m. Now observe,
(⟨0|b⊗In) (⟨0| a⊗Im)Uψ|0⟩a+m= (⟨0| b⊗In) (|ψ⟩ m− |E ψ⟩m)/α(B.35)
= (⟨0| b⊗In) (Vϕ|0⟩m+|E v⟩m− |E ψ⟩m)/α(B.36)
= ((|ϕ⟩ n− |E ϕ⟩n)/β+ (⟨0| b⊗In)(|E v⟩m− |E ψ⟩))/α.
(B.37)
Consequently, since(⟨0| b⊗In)(⟨0| a⊗Im) =⟨0| a+b⊗In,
∥|ϕ⟩n−αβ(|0⟩ a+b⊗In)Uψ|0⟩a+b+n∥2(B.38)
≤ ∥|E ϕ⟩n∥2+β∥|E ψ⟩m∥2+β∥|E v⟩m∥2≤δ+β(ϵ+γ).(B.39)
Lemma B.11(Tracing Out Qubits in Vector Sub-Encodings).Let Ube an (α, a, ϵ) -VE for |0⟩b|ψ⟩n.
Then,Uis an(α, a+b, ϵ)-VE for|ψ⟩ n.
Proof. Let|E⟩b+n:=|0⟩ b|ψ⟩n−α(⟨0| a⊗In+b)U|0⟩ a+b+n . Since ⟨0|a+b⊗In= (⟨0| b⊗In)(⟨0| a⊗
Ib+n),(⟨0| a+b⊗In)U|0⟩ a+b+n =1
α(|ψ⟩n−(⟨0| b⊗In)|E⟩ b+n). Thus,
∥|ψ⟩ n−α(⟨0| a+b⊗In)U|0⟩ a+b+n∥2=∥(⟨0| b⊗In)|E⟩ b+n∥2≤ϵ.(B.40)
Proof of Lemma 3 (Vector Tensor Product). This result closely follows the derivation of the tensor
product of block-encodings (Lemma B.2), which was a rederivation of Lemma 1 of Camps and
Van Beeumen [83].
Uψacts on an a-qubit ancilla register and a n-qubit main register, while Uϕacts on an b-qubit ancilla
register and am-qubit main register.
As per Lemma B.2, define Πto swap the n-qubit register with the b-qubit register acting trivially on
the other two registers. Again, Πhas a circuit depth bounded by O(max(n/b, b/n))∈O(max(n, b)) .
Then, (⟨0|a+b⊗In+m)Π†= (⟨0| a⊗In)⊗(⟨0| b⊗Im). Let V= Π†(Uψ⊗Uϕ). Let|Eψ⟩n=
|ψ⟩n−α(⟨0| a⊗In)Uψ|0⟩a+nand|E ϕ⟩m=|ϕ⟩ m−β(⟨0| b⊗Im)Uϕ|0⟩b+m. Then,
(⟨0|a+b⊗In+m)Π†(Uψ⊗Uϕ)|0⟩a+b+n+m =1
αβ(|ψ⟩n− |E ψ⟩n)⊗(|ϕ⟩ m− |E ϕ⟩m),(B.41)
25

[[PAGE 26]]
and so,
∥|ψ⟩ n|ϕ⟩m−αβ(⟨0| a+b⊗In+m)Π(U ψ⊗Uϕ)|0⟩a+b+n+m ∥2≤ϵ+δ+ϵδ.(B.42)
Proof of Lemma 4 (Vector Concatenation). We now present the proof of a simple result on the
concatenation of vectors stored in VEs. This result follows from a simple modification of LCU [ 87].
In essence, given a set of D= 2dvectors {|ψj⟩n}j, we first create vector encodings of {|j⟩d|ψj⟩n}j
and then take the resulting sum of the encoded vectors following LCU, yielding an encoding of
(⟨ψ0|n. . .⟨ψ D−1|n)†.
For allj, define|E ψj⟩n:=|ψ j⟩n−α(⟨0| a⊗In)Ui|0⟩a+n.
First, let jbedbits, and let j=j 0j1. . . jd−1. Define Xj:=Xj0⊗Xj1⊗. . .⊗Xjd−1. Note that
|j⟩d=X j|0⟩d, and thus that Xjis a(1,0,0) -VE for |j⟩d. Then, we can invoke Lemma 3 with Uj
andXjto obtain Vj, an(αj, a, ϵ) -VE for |j⟩d|ψj⟩nwithO(T+n) circuit complexity. Moreover, by
inspecting Lemma 3, we find that(⟨0| a⊗In+d)Vj|0⟩a+d+n =1
αj(|j⟩d|ψj⟩n− |j⟩ d|Eψj⟩n).
Additionally, define S:=PD−1
j=0|j⟩⟨j| d⊗Vj. This can be implemented by a sequence of O(D)
multi-controlled gates, each enactingV jwhen the control register is|j⟩ d(in the standard fashion of
LCU [ 87]). First, note that by using Saeedi and Pedram [88] a multiple-controlled gate with O(d)
controls can be split into a sequence of O(d) single and two-qubit gates. By splitting each of the d
control qubits into a+d+n copies (with O(log(a+d+n)) depth), we can control each gate in
each layer of Ujin parallel with O(d) circuit depth. Since these ancillas can be uncomputed and
traced out, we ignore them in the complexity analysis. Thus, each multi-controlled Vjgate can be
decomposed into a sequence of O(dT) single and two-qubit gates. Thus, Shas a total circuit depth
ofO(dDT). Let ˆH:=H⊗d⊗Id+n+a . Using⟨0| a+d⊗In+d= (⟨0| d⊗In+d)(Id⊗ ⟨0| a⊗Id+a),
(⟨0|d+a⊗In+d)ˆHSˆH|0⟩ 2d+a+n (B.43)
= (⟨+| d⊗In+d)(Id⊗ ⟨0| a⊗In+d)D−1X
j=0(|j⟩⟨j| d⊗Vj)|+⟩ d|0⟩a+d+n (B.44)
=1
DD−1X
j=0|j⟩d|ψj⟩d− |j⟩ d|Eψj⟩n
αj.(B.45)
Then, noting thatN2=PD−1
j=01
α2
j, and thatPD−1
j=0|j⟩d|Eψj⟩n/αj
2≤ Nϵ,
|Ψ⟩d+n
N−D
N(⟨0|d+a⊗In+d)ˆHSˆH|0⟩ 2d+a+n
2=1
ND−1X
j=0|j⟩d|Eψj⟩n/αj
2≤ϵ.(B.46)
Thus, ˆHSˆHis a(D/N, d+a, ϵ)for|Ψ⟩d+n
NwithO(dDT)circuit complexity.
B.1 GENERALMATRIX-VECTOR-SQUAREDPRODUCT
In this subsection, we will derive a procedure which given anarbitrarymatrix Wand quantum
state|ψ⟩, allows for a state proportional to the product of W(|ψ⟩)2to be obtained with complexity
independentof the Frobenius norm (and thus rank), and sparsity, of W. To the best of our knowledge,
this is the first result which allows such a product without either a rank or sparsity condition on
W. The key insight is to avoid ever constructing a block-encoding of the operator W, and directly
query its columns weighted by the entries of the vector it is being applied to. In particular, at
a high-level we construct two objects. Define the columns of W= ( w0. . .w N−1), define
the column norms aj:=∥w j∥2, and the normalized versions of the columns |wj⟩n=w j/aj.
Additionally, define the state we are applying it to as |ψ⟩n=P
jψj|j⟩n. First, we construct the
normalized stateP
jψj|j⟩n|wj⟩n. Clearly, this object has no Frobenius norm dependence. We
would like to map all the vectors in the first register to the |0⟩state so that we have something
26

[[PAGE 27]]
resembling the matrix-vector product, and to do this we construct another operator. Note that the
matrix Q=
a0In. . . a N−1In
0
(i.e., the first Nrows are non-zero, and the rest are all zero)
when applied to |ϕ⟩2n=P
jψj|j⟩n|wj⟩nyields Q|ϕ⟩ 2n=|0⟩ n⊗(W|ψ⟩ n). However, this object
has a spectral norm Ω(∥W∥F). Instead, we define M:=
a0ψ0In. . . a N−1ψN−1In
0
and note
thatMcan be shown to have ∥M∥2≤1, and moreover, we subsequently show how a block-encoding
of this operator can be efficiently obtained. Consequently, since M|ϕ⟩ 2n=|0⟩ n⊗(W(|ψ⟩ n)2), the
result follows. The rest of this section simply derives the ingredients necessary to rigorously prove
this intuition.
Definition B.2( RY(t)Gate).Let t∈R , and let Ybe the standard single-qubit Pauli- Ygate. Then,
define
RY(t) :=e−itY= cos(t)I−isin(t)Y=
cos(t)−sin(t)
sin(t) cos(t)
.(B.47)
For completeness, we will now present a standard result allowing one to transfer digitally represented
information to the amplitudes of a quantum state.
Lemma B.12( CRY(t)Gate).Let t∈R . Let Ybe a standard Pauli- Ygate. Let |a⟩dbe ad-bit
standard basis vector, and let |ψ⟩1be an arbitrary single-qubit quantum state. Then, we can define
the gateCR Y(t)by the following action,
CRY(t)|ψ⟩ 1|a⟩d= (e−iatY|ψ⟩1)|a⟩d.(B.48)
In the event that|ψ⟩ 1=|0⟩ 1, this action can be simplified to
CRY(t)|0⟩ 1|a⟩d= (cos(at)|0⟩ 1+ sin(at)|1⟩ 1)|a⟩d.(B.49)
Moreover, theCR Y(t)gate is implemented withO(d)circuit depth.
Proof. This is a standard result. This proof is included for completeness, and follows the one in Rattew
and Koczor [89]. LetD= 2d. First, note that CRY(t) =PD−1
a=0e−iatY⊗ |a⟩⟨a| . Additionally, let
a=a d−1ad−2. . . a 1a0=ad−12d−1+...+a 12 +a 0. Then,
e−iatY=e−i(a d−12d−1+...+a 12+a 0)tY=e−iad−12d−1tY·. . .·e−ia 1tYe−ia 0tY.(B.50)
Then, CRY(t)can be implemented by applying a sequence of dcontrolled e−i2jtYgates (Defini-
tion B.2), targeting the first register, controlled on thejthbit of the second register.
We now present a result on obtaining a block-encoding of an arbitrary diagonal matrix whose entries
are stored in QRAM. This is essentially a special case of Lemma 48 of Gilyén et al. [36], but by
considering this special case moderate improvements in complexity can be obtained.
Lemma B.13(Quantum Block-Encoding of Diagonal Matrices from QRAM).Let N= 2n. We
are given a set of Nreal coefficients, {aj}jsuch that ∀j,|a j| ≤1 . Assume that each ajcan
be represented exactly in a binary encoding with d-bits of precision, and define D= 2d. Define
bj:= arccos(a j)D/π , and for simplicity assume that each bjcan also be implemented with exactly
d-bits of precision4, and note that bj∈[D] . Assume that we are given an oracle, implemented via
QRAM, such that U|0⟩ d|j⟩n=|b j⟩d|j⟩n. Then, we can obtain UA, a(1, d+ 1,0) -block-encoding
forA=diag(a 0, . . . , a N−1), withO(dn)circuit depth.
Proof. Define the circuit V:= (I 1⊗U†)(CR Y(π
D)⊗I n)(I1⊗U) , with CRY(π
D)defined as
per Lemma B.12. First, since for any |ϕ⟩and basis vector |j⟩,|ϕ⟩⊗|j⟩⟨j|= (|ϕ⟩|j⟩)⟨j| , observe that
(I1⊗U)(|0⟩ d+1⊗In) =N−1X
j=0[(I1⊗U)|0⟩ 1|0⟩d|j⟩n]⟨j|n=N−1X
j=0(|0⟩1|bj⟩d|j⟩n)⟨j|n.(B.51)
4In practice this will result in an additional logarithmic source of error, which we are neglecting, as it is akin
to finite-precision arithmetic error which is usually neglected in classical algorithm analysis.
27

[[PAGE 28]]
Then, sincecos(b jπ
D) = arccos(a j),
(CR Y(π
D)⊗I n)(I1⊗U)(|0⟩ d+1⊗In) =N−1X
j=0
(aj|0⟩1+q
1−a2
j|1⟩1)|bj⟩d|j⟩n
⟨j|n.
(B.52)
Then, since (⟨0|d+1⊗In)(I1⊗U†) = [(I 1⊗U)(|0⟩ d+1⊗In)]†=PN−1
j=0|j⟩n(⟨0|1⟨bj|d⟨j|n), we
readily find that
(⟨0|d+1⊗In)V(|0⟩ d+1⊗In) =N−1X
j=0aj|j⟩⟨j|=diag(a 0, . . . , a N−1) =A.(B.53)
Thus, Vis a(1, d+ 1,0) -block-encoding for A. The circuit depth of implementing Uis the
depth of making a QRAM query, and is thus O(dlogN) =O(nd) (see Definition A.1). The cost
of implementing the CRYgate is simply O(d) as per Lemma B.12, and thus the overall circuit
complexity of this block-encoding isO(nd).
In the case where each aj∈C, the complex and real parts need to be specified separately. A diagonal
block-encoding of the real and imaginary parts can then be obtained using Lemma B.13, and can then
be summed by adding an ancilla to obtain a (2, d+ 2,0) -block-encoding with the same overall circuit
complexity. One might wonder why, given a QRAM assumption, a state-preparation unitary yielding a
state proportional toP
jaj|j⟩can’t be used instead, in combination with the diagonal block-encoding
of state amplitudes result of Rattew and Rebentrost [58]. If each ajrepresent the column norm of some
matrix W, doing so would result in a normalization factor ofP
jaj|j⟩
2=qP
j|aj|2=∥W∥F,
yielding a Frobenius norm-dependence which this approach avoids.
The following data-structure is useful in situations where you are willing to pay a pre-processing cost
linear (up to polylogarithmic factors) in the number of non-zero matrix elements, but want a fast
algorithm at runtime. This is the case with accelerating neural network inference. The following data
structure is very similar to the one given in Kerenidis and Prakash [81].
Definition B.3(Preprocessed Matrix QRAM Data Structure).LetN= 2n, and letD= 2d.
LetW∈CN×Nand let ∥W∥2≤1 . Let the columns of Wbe represented as W=
(w0. . .w N−1). Additionally, define |wj⟩=w j/∥w j∥2, and aj=∥w j∥. Let bj:=
arccos(a j)D/π . For simplicity, we assume that bjcan be exactly written with d-bits, and thus
thatbjwill be an integer between [0, D−1] . We say we have access to a Preprocessed QRAM Data
Structure forWif we have a QRAM oracleU W(as per Definition A.2) such that
UW|j⟩n|0⟩n=|j⟩ n|wj⟩n,(B.54)
and we also have access to a QRAM yielding the mapping,
UA|0⟩d|j⟩n=|bj⟩d|j⟩n.(B.55)
UWcan be implemented with O(log2N)circuit depth, and with ˜O(N2)total qubits (as per Defini-
tion A.2). UAcan be implemented with O(dlogN) circuit depth, and with ˜O(dN) total qubits (as
per Definition A.1).
We are now ready to present a somewhat surprising result on matrix-vector multiplication with
arbitrary (potentially full-rank and dense) matrices and the element-wise square of a given vector.
The following uses ideas similar to importance-sampling.
Theorem B.1(Product of Arbitrary Matrix with a Vector Element-wise Squared).Let N= 2n.
We are given a matrix W∈CN×Nthrough the data-structure in Definition B.3. Let dbe the
number of bits required to represent the function of the column norms of W,bj, as per Definition B.3.
Additionally, we are given the unitary Uψwith circuit complexity O(Tψ), a(α, a, ϵ) -VE for the
quantum state |ψ⟩n. Define the function g:C7→R asg(x) =|x|2, andN:=∥Wg(|ψ⟩ n)∥2. Then
we can construct the unitary Ufwhich is a (α2
N,2a+d+ 3 +n,2αϵ
N)-VE for Wg(|ψ⟩ n)/N, and
has a circuit depth ofO(T ψ+dn+n2).
28

[[PAGE 29]]
Proof. Noting that aj=∥W|j⟩∥2, it is easy to show ∥W∥2≤1 =⇒ ∀j, a j≤1;aj=∥W|j⟩∥2≤
maxx:∥x∥2=1∥Wx∥2=∥W∥2≤1. Consequently, by Lemma B.13 we can immediately get UA, a
(1, d+ 1,0)-block-encoding forA=diag(a 0, . . . , a N−1)withO(dn)circuit depth.
Let|ψ1⟩n:=A|ψ⟩ n=PN−1
j=0ajψj|j⟩n,N1:=∥|ψ 1⟩n∥2. By Lemma 2, we can combine UA
andUψto obtain V1, a(α/N 1, a+d+ 1, ϵ/N 1)-VE for |ψ1⟩n/N1. This has circuit complexity
O(Tψ+dn).
By Lemma B.6, we can get U0, a(1,2,0) -block-encoding for the n+a+d+1 -qubit projector |0⟩⟨0| .
Let|Eψ1⟩n:=|ψ1⟩
N1−α
N1(⟨0|a+d+1 ⊗In)V1(|0⟩n+a+d+1 ). Then, by Definition 3, ∥|Eψ1⟩n∥2≤
ϵ/N 1. Moreover, ⟨0|a+d+1 ⊗In)V1(|0⟩n+a+d+1 ) =1
α(|ψ1⟩n− N 1|Eψ1⟩n). Then, observe that
V2:=U 0(I2⊗V†
1)is a(1,2,0) -block-encoding for |0⟩⟨0|V†
1. Let c=a+d+ 1 . Noting that
(|0⟩c+2⊗In) = (|0⟩ 2⊗Ic+n)(|0⟩ c⊗In), then,
(⟨0|c+2⊗In)V2(|0⟩c+2⊗In) = (⟨0| c⊗In)(⟨0| 2⊗Ic+n)V2(|0⟩2⊗Ic+n)(|0⟩ c⊗In)(B.56)
= (⟨0| c⊗In)|0⟩⟨0|V†
1(|0⟩c⊗In)(B.57)
=1
α(|0⟩n(⟨ψ1|n− N 1⟨Eψ1|n)).(B.58)
The third inequality follows by noting that (⟨0|c⊗In)|0⟩n+c=|0⟩ n, and that by Definition 2,
(⟨0|2⊗Ic+n)V2|0⟩2⊗Ic+n) =|0⟩⟨0|V†
1. Then, letting|0⟩⟨ψ 1|be a2n×2nprojector,
∥|0⟩⟨ψ 1| −α(⟨0| c+2⊗In)V2(|0⟩c+2⊗In)∥2=N 1∥|0⟩⟨E ψ1|∥2≤ϵ.(B.59)
Consequently, V2is a(α, a+d+ 3, ϵ) -block-encoding for the 2n×2nprojector |0⟩⟨ψ 1|. Moreover,
the circuit complexity of V2is dominated by the circuit complexity of V1, and thus is O(Tψ+dn) .
Then,V 3:=V 2⊗Inis a(α, a+d+ 3, ϵ)-block-encoding for(|0⟩⟨ψ 1|)⊗I n.
LetU Wbe defined as in Definition B.3, i.e., it enactsU W|j⟩n|0⟩n=|j⟩ n|wj⟩n.
Define|ϕ⟩ 2n:=PN−1
j=0ψj|j⟩n|wj⟩n.
Then, letS:= (I a⊗UW)(Uψ⊗In). We will now show thatSis an(α, a, ϵ)-VE for|ϕ⟩ 2n.
Let|E ψ⟩n:=|ψ⟩ n−α(⟨0| a⊗In)Uψ|0⟩a+n, thus,(⟨0| a⊗In)Uψ|0⟩a+n=1
α(|ψ⟩n− |E ψ⟩n)
Moreover, define the a-qubit projector, pa
j:=|j⟩⟨j| . Then, Ia+n=P2a−1
j=0pa
j⊗In. Finally, define
|γj⟩n:= (⟨j| a⊗In)Uψ|0⟩a+n. Of course,
Uψ|0⟩a+n=
2a−1X
j=0pa
j⊗In
Uψ|0⟩a+n=1
α(|0⟩a(|ψ⟩n− |E ψ⟩n)) +2a−1X
j=1|j⟩a|γj⟩n.(B.60)
Consequently,
(⟨0|a⊗I2n)S|0⟩ a+2n = (⟨0| a⊗I2n)(Ia⊗UW)(Uψ⊗In)|0⟩a+2n (B.61)
= (⟨0| a⊗UW)
1
α(|0⟩a(|ψ⟩n− |E ψ⟩n)) +2a−1X
j=1|j⟩a|γj⟩n
|0⟩n(B.62)
=1
α(|ϕ⟩2n−UW|Eψ⟩n|0⟩n).(B.63)
Thus,
∥|ϕ⟩ 2n−α(⟨0| a⊗I2n)S|0⟩ a+2n∥2=∥U W|Eψ⟩n|0⟩n∥2≤ϵ.(B.64)
Thus, Sis an (α, a, ϵ) -VE for |ϕ⟩2n. Moreover, the circuit complexity of Scomes from summing
the circuit complexity of UψandUW. As per Definition B.3, the circuit complexity of UWisO(n2),
giving an overall circuit complexity forSofO(T ψ+n2).
Define|Γ⟩ n:=Wg(|ψ⟩ n), and note that
[(|0⟩⟨ψ 1|)⊗I n]|ϕ⟩2n=|0⟩ nN−1X
j=0|ψj|2aj|wj⟩n=|0⟩ n|Γ⟩n.(B.65)
29

[[PAGE 30]]
We now have V3, a(α, a+d+ 3, ϵ) -block-encoding for (|0⟩⟨ψ 1|)⊗I n, and San(α, a, ϵ) -VE for
|ϕ⟩2n. We will now invoke Lemma 2 to take the product of the matrix encoded in V3with the vector
encoded in S, and then will invoke Lemma B.11 to remove the |0⟩ntensored register. This yields Uf,
an(α2
N,2a+d+ 3 +n,2αϵ
N)-VE for|Γ⟩ n/Nwith circuit complexityO(T ψ+dn+n2).
B.2 CONVOLUTIONBLOCK-ENCODING
In this section, we will first provide a matrix-form of a 2D multi-filter convolution (with stride 1
and0padding to ensure the input and outputs have the same dimension). We then derive a quantum
block-encoding of the matrix form of the convolution.
As a note, some popular deep learning frameworks such as PyTorch [ 90] actually implement cross-
correlation rather than convolution. However, in the pre-processing stage, our convolutional block-
encoding immediately gives a cross-correlation block-encoding by simply switching the Qoperator
(Definition B.6) with a QToperator. Finally, in this section, we assume that all addition on basis
vectors is mod the dimension of the vector. I.e., for integers i, j,|i+j⟩ n=|(i+j)modN⟩ n(with
N= 2n).
Definition B.4(Permutation Matrix).Define the following Ndimensional unitary permutation
matrix that maps an input basis vectorito the basis vector(i+ 1)modN.
P:=NX
i=0|i+ 1⟩⟨i|=
0 0 0. . .1
1 0 0 0
0 1 0 0
.........
0 0 0. . .0
.(B.66)
Definition B.5( RZPhase Gate).Define the single-qubit phase gate, RZ(t) :=eitZ=
eit0
0e−it
.
We now derive a block-encoding of the permutation matrix Pacting on mqubits. We include
this result for completeness, and similar results may be found in the literature (see e.g., Motlagh
and Wiebe [91], where they derive a 1D circulant convolution via QSP, or Camps et al. [92]). Our
implementation ofPmis identical to a+madder implemented with QFT, see e.g., Draper [93].
Lemma B.14(Permutation Matrix Block-Encoding).Let m∈N >0. Let N= 2n. The mth
power of the permutation matrix Pis given by Pm=PN−1
j=0|j+m⟩⟨j| . Then, we can get a
(1,1,0)-block-encoding withO(n2)circuit complexity forPm.
Proof. Drawing inspiration from Motlagh and Wiebe [91], Sedghi et al. [94], letF:=QFT
represent the Quantum Fourier Transform on nqubits. Define ωj
N:=e2πij/N. Noting that F=
1√
NPN−1
i=0PN−1
j=0ωij
N|i⟩⟨j| , it is easy to show that PmF|j⟩=ω−mj
NF|j⟩ . Consequently, we can
write Pm=FDF−1, where D=diag(ω0
N, ω−m
N, . . . , ω−m(N−1)
N ). Thus, by getting a block-
encoding of D, we can implement Pmby taking a product of FDF−1. Let|j⟩nbe a basis vector,
and let j= 2n−1jn−1+. . .+ 21j1+ 20j0. We will now give a unitary Vmwhich implements the
mapping Vm|0⟩1|j⟩n=ω−jm
N|0⟩1|j⟩n. Noting that ω−jm
N=e−2πijm/N=Qn−1
l=0e−2πim(2jljl)/N,
we can apply a sequence of ncontrolled RZ(t)gates, where the lthgate is controlled on bit jl
and applies RZ(m2jl/N) on the ancilla qubit. This implements the desired mapping, and can be
easily shown to be a (1,1,0) -block-encoding for D. The Quantum Fourier Transform [ 95] can be
implemented with O(n2)circuit complexity [ 14], and so we can get a trivial (1,0,0) -block-encoding
for both FandF†. Thus, (I1⊗F)V m(I1⊗F†)is a(1,1,0) -block-encoding for Pm, with O(n2)
circuit depth. Its worth noting that since the ancilla qubit in Vmis separable after the computation,
this could be equivalently considered a(1,0,0)-block-encoding.
30

[[PAGE 31]]
Definition B.6(Discrete Unilateral Shift Operator).Define Qto be the N-dimensional discrete
unilateral shift operator,
Q:=N−2X
j=0|j+ 1⟩⟨j|=
0 0. . .0 0
1 0 0 0
0 1 0 0
.........
0 0. . .1 0
.(B.67)
This is just the permutation matrixPwithout wrap-around.
Lemma B.15(Block-Encoding of Q).Let N= 2n. Define Qas per Definition B.6. Then, we can
obtain a(1,4,0)-block-encoding forQwithO(n2)circuit complexity.
Proof. By Lemma B.14, we can obtain a UPa(1,1,0) -block-encoding of P=PN−1
j=0|j+ 1⟩⟨1|
withO(n2)circuit complexity. By Lemma B.6, we can obtain Va(1,2,0) -block-encoding of
then-qubit projector |0⟩⟨N−1| withO(n) circuit depth. Following LCU [ 87,36], we can get
the sum of these two block-encodings, introducing an additional ancilla, with the circuit Uf:=
(H⊗I 2+n)(|0⟩⟨0| 1⊗I1⊗UP−|1⟩⟨1| 1⊗V)(H⊗I 2+n). Then, Ufis a(1,3,0) -block-encoding for
1
2(P−|0⟩⟨N−1|) =1
2Q, with O(n2)circuit complexity. Noting that Q†Q=I n−|N−1⟩⟨N−1| ,
it is clear that ∥Q∥2≤1. Moreover, since all the singular values of Q/2 are either 0or1/2, we can
invoke Lemma B.4, a special case of oblivious amplitude amplification [ 36], to immediately convert
this to a(1,4,0)-block-encoding forQwith only 3 calls toU f.
Lemma B.16(Block-Encoding of Qm).Let m∈N >0and let N= 2n. Define the N-dimensional
operator Qas per Definition B.6. Then, we can obtain a (1,4m,0) -block-encoding of Qmwith
O(mn2)circuit complexity.
Proof. As per Lemma B.15, we can obtain UQa(1,4,0) -block-encoding for QwithO(n2)circuit
complexity. Invoking Lemma 53 (Product of Block-Encoded Matrices) of Gilyén et al. [36] withUQ
mtimes directly yields a(1,4m,0)-block-encoding ofQmwithO(mn2)circuit complexity.5
Now, we present a standard well-known result giving the matrix form of a 2D multi-filter convolution
(see e.g., Sedghi et al. [94], Kerenidis et al. [41]).
Lemma B.17(Matrix Form of 2DMulti-Filter Convolution).Let M= 2m, letn= 2m , let
N= 2n, and let D= 2d. Let C= 2crepresent the number of input and output channels. Let X
represent the rank −3input tensor, which in vectorized form (stored in column-major order for each
input channel) is given by, |X⟩n+c=PC−1
i=0PM−1
j=0PM−1
k=0Xi,k,j|i⟩c|j⟩m|k⟩m.I.e.,|X⟩n+cis of
dimension M2C=NC . Define ˜Xi,j,k=X i,j,kifj≥0 andk≥0 , and ˜Xi,j,k= 0otherwise. We
can define the convolutional kernel Kto be a rank- 4tensor containing each of the C,C×D×D
filters6, where the first index represents the output channel, the second index represent the input
channel, the third index represents the row index, and the fourth index represents the column index.
Then, entryy, zof thexthoutput channel after convolution withKis given by,
[X∗K] x,y,z:=C−1X
j=0D−1X
k=0D−1X
l=0Kx,j,k,l˜Xj,z−k,y−l .(B.68)
DefiningQas per Definition B.6, we can give the matrix form of the convolution,
C:=C−1X
i=0C−1X
j=0D−1X
k=0D−1X
l=0Ki,j,k,l(|i⟩⟨j| c⊗Ql⊗Qk).(B.69)
I.e.,C|X⟩ n+c=vec(X∗K).
5This can likely be optimizing by using QSVT [36].
6If the number of channels is1(i.e.,C= 1), then the kernel isD×Ddimensional.
31

[[PAGE 32]]
Proof. We will verify that Cindeed implements the mapping specified in Equation (B.68) by comput-
ing the following, ⟨x|c⟨y|m⟨z|mC|X⟩ n+c. Note that for all i < l ,⟨i|Ql= 0, and that for all i≥l ,
⟨i|Ql=⟨i−l| . Consequently, if y−l≥0, z−k≥0 , then⟨j|c⊗(⟨y| m⟨z|mQl⊗Qk)|X⟩ n+c=
Xj,z−k,y−l , and if y−l <0 orz−k <0 then⟨j|c⊗(⟨y| m⟨z|mQl⊗Qk)|X⟩ n+c= 0. Thus,
⟨j|c⊗(⟨y| m⟨z|mQl⊗Qk)|X⟩ n+c=˜Xj,z−k,y−l . Therefore,
⟨x|c⟨y|m⟨z|mC−1X
j=0Ki,j,k,l(|i⟩⟨j| c⊗Ql⊗Qk)|X⟩ n+c=C−1X
j=0Kx,j,k,l˜Xj,z−k,y−l .(B.70)
As a result,
⟨x|c⟨y|m⟨z|mC|X⟩ n+c=C−1X
j=0D−1X
k=0D−1X
l=0Kx,j,k,l˜Xj,z−k,y−l = [X∗K] x,y,z.(B.71)
Proof of Lemma 5. Define |X⟩n+c,K, andCas per Lemma B.17. As a result, obtaining a block-
encoding ofCallows us to implement the desired2Dconvolution in the vectorized setting.
First, for a given i, j, k, l , we will show how to obtain a block-encoding of Ki,j,k,l(|i⟩⟨j| c⊗Ql⊗Qk).
Using Lemma B.6, we can obtain Ui,ja(1,2,0) -block-encoding of the c-qubit projector |i⟩⟨j| c, with
O(c) circuit depth. Then, using Lemma B.16, we can obtain UQla(1,4l,0) -block-encoding of m
qubit QlwithO(lm2)circuit complexity. We similarly obtain UQka(1,4k,0) -block-encoding of
mqubit QkwithO(km2)circuit complexity. We can then invoke Lemma B.2 with Ui,jandUQl,
and again with UQk, to obtain Ui,j,l,k , a(1,2 + 4l+ 4k,0) -block-encoding of |i⟩⟨j| c⊗Ql⊗Qk
withO(c+Dm2)circuit complexity. To make each operator act on the same number of qubits, we
will augment each with the appropriate number of tensored identities to yield a (1,2 + 8D,0) -block-
encoding for the corresponding operator.
Define|K⟩ 2c+2d :=PC−1
i=0PC−1
j=0PD−1
k=0PD−1
k=0Ki,j,k,l|i⟩c|j⟩c|k⟩d|l⟩d, and define|√
K⟩2c+2d =
p
|K⟩2c+2d (with the square-root applied element-wise). Then, define NK:=|√
K⟩2c+2d
2=
∥|K⟩ 2c+2d∥1/2
1, and|K⟩2c+2d :=|K⟩ 2c+2d /NK. Noting that this vector is C2D2dimensional,
we can brute-force construct a unitary UK, with a total of O(C2D2)single and two qubit gates,
such that UK|0⟩2c+2d =|√
K⟩2c+2d /NK[64]. We can then invoke Lemma B.5, obtaining a
(N2
K,2 + 8D+ 2 log(CD),0) -block-encoding for CwithO(cdC2D3m2)circuit complexity. This
is equivalent to a (1,2 + 8D+ 2 log(CD),0) -block-encoding for C/∥|K⟩ 2c+2d∥1. Since we are
concerned with accelerating inference, we will ignore classical pre-computation costs that must
only be paid one time to construct this datastructure. We can then invoke Lemma B.3, setting
γ=∥|K⟩ 2c+2d∥1/2∥C∥2andδ= 1/2 , since ∥C/∥|K⟩ 2c+2d∥1∥2≤1
22∥C∥2
∥|K⟩ 2c+2d∥1. Neglecting
the logarithmic error-terms incurred by Lemma B.3 (as these will not dominate complexity), this
then yields a (1,3 + 8D+ 2 log(CD),0) -block-encoding forC
2∥C∥2withO(∥|K⟩ 2c+2d∥1
∥C∥2cdC2D3m2)
circuit depth. We will now show that∥|K⟩ 2c+2d∥1
∥C∥2≤DC3/2, and thus that the overall circuit depth is
bounded byO(cdm2C3D4).
We will now upper-bound ∥|K⟩ 2c+2d∥1. Define the basis vector |x⟩c+2m =|x 1⟩c|x2⟩m|x3⟩m.
Then, the xthrow of Cis given by ⟨x|c+2mC. Simple analysis shows that ⟨x|c+2mC=PC−1
j=0PD−1
k=0PD−1
l=0Kx1,j,k,l⟨j|c⊗ ⟨x 2−l|m⊗ ⟨x 3−k| m, where ⟨x2−l|m= 0 ifx2−l <0
and⟨x3−k| m= 0 ifx3−k= 0 . Then it can be readily shown that ∥⟨x|c+2mC∥2
2=PC−1
j=0PD−1
k=0PD−1
l=0|Kx1,j,k,l|2. For any operator Awith∥A∥2≤1, the maximum column
norm max|i⟩∥A|i⟩∥2≤max |ψ⟩:∥|ψ⟩∥2=1∥A|i⟩∥2≤1. Similarly, since ∥A∥2=A†
2, the max-
imum row norm cannot exceed the spectral norm of the matrix. Therefore, any row of Cmust
have ℓ2-norm bounded by ∥C∥2, thus,PC−1
j=0PD−1
k=0PD−1
l=0|Kx1,j,k,l|2≤ ∥C∥2
2. Consequently,
∥|K⟩ 2c+2d∥2
2=PC−1
i=0PC−1
j=0PD−1
k=0PD−1
l=0|Ki,j,k,l|2≤C∥C∥2
2. Moreover, for an n-dimensional
32

[[PAGE 33]]
vector x,∥x∥1≤√n∥x∥2, and thus, ∥|K⟩ 2c+2d∥1≤√
C2D2√
C∥C∥2=DC3/2∥C∥2. Conse-
quently,∥|K⟩ 2c+2d∥1/∥C∥2≤DC3/2.
To see a set of related block-encoding circuits, see Camps et al. [92].
It is also worth noting that the preceding result can be made substantially more efficient by utilizing a
circulant convolution to implement the non-circulant convolution. We will now quickly sketch this
idea for future optimization. For simplicity, we assume that the convolution has one input channel and
one output channel, i.e., that the input is a rank-2 tensor (e.g., a black and white image). Let M= 2m.
Then, if the input image is X∈RM×M, we can add 0padding with the M×M projector, |0⟩⟨0| m⊗X.
Then, enacting a circulant convolution on this augmented operator and projecting onto the zero-state
of the first register yields the desired non-circulant convolution. Moreover, we can define a circulant
2Dconvolution as [X∗K] i,j=Pl−1
k=0Pd−1
l=0Kk,lXi−k,j−l . The following sketch generalizes the
1D circulant convolution given in Motlagh and Wiebe [91], and also follows the ideas discussed
in Sedghi et al. [94]. Consequently, the operator C:=Pd−1
i=0Pd
j=0Ki,jPj⊗Piimplements X∗K
in the vectorized setting (using a column-major vectorization for X). Let ωM:= exp(2πi/M) be
theMthroot of unity. Let F:=QFT represent the Quantum Fourier Transform on mqubits. It is
easy to show that PkF|j⟩=ω−kj
MF|j⟩ . Thus, let D:=F−1PF=diag(ω0
M, ω−1
M, . . . , ω−(M−1)
M ).
Consequently, P=FDF−1, and so C= (F⊗F)Pd−1
i=0Pd−1
j=0Ki,jDj⊗Di
(F−1⊗F−1).
Clearly, since implementing the QFT is efficient on a quantum computer, the key to implementing C
is in implementing a block-encoding of the diagonal matrix Γ :=Pd−1
i=0Pd−1
j=0Ki,jDj⊗Di. Noting
that this is a 1-sparse matrix with efficiently computable entries, a technique such as Gilyén et al.
[36] can be immediately used to obtain the desired block-encoding (replacing QRAM assumptions
with arithmetic oracles computing the locations and values of the non-zero elements). This can be
further optimized by replacing the arithmetic with QRAM. In the multi-filter case, the diagonal matrix
becomes a block-diagonal matrix (with blocks of height and width given by the number of input and
output channels), and the sparse block-encoding techniques can still be used.
B.3 NON-LINEARTRANSFORMATION OFVECTOR-ENCODINGS
We now present an essential result on transforming the amplitudes of a state encoded as a VE. This
result is a direct translation of the ideas in the result given in Rattew and Rebentrost [58] (which
in turn builds on Guo et al. [96], Mitarai et al. [76]) to the setting of VEs. While Rattew and
Rebentrost [58] also give a similar result in the setting of a VE (called an SPBE in that paper),
they obtain it by treating the whole unitary VE as a state-preparation unitary, and then invoke their
non-linear amplitude transformation (NLAT) result on that, which gives slightly worse complexity
than just directly re-deriving the whole transformation result in the framework of VEs. We include
the following for completeness and simplicity, and do not claim novelty on this result.
Lemma B.18(NLAT of VE [ 58]).Let N= 2n. Let 0≤ϵ 0≤1, and α≥1 . We are given a
unitary matrix Uψwhich is an (α, a, ϵ 0)-VE for the n-qubit real quantum state |ψ⟩nwith circuit
complexity O(T) , and a function f:R7→R with Lipschitz constant Lsuch that f(0) = 0 . Define ϵ1
such that 0< ϵ 1≤L. Define N:=∥f(|ψ⟩ n/α)∥2. Define the interval of approximation [−τ, τ] ,
where 0< τ≤1 which can be set to either τ= 1 or any value such that τ≥1+ϵ 0
αif a smaller
region of approximation yields a better complexity. Define the polynomial P:R7→R , such that
with degree k,max x∈[−τ,τ] |P(x)−f(x)| ≤Lϵ1
2√
N. Suppose we are given a bound ˜γsatisfying
˜γ≥max x∈[−1,1] |P(x)/x| , and require that P(0) = 0 . Then, we can obtain a unitary circuit Uf
that is a
4˜γ
N, n+ 2a+ 4,L
N(ϵ0+ϵ1)
-VE for f(|ψ⟩ n/α)/N , and which requires O(k) calls to a
controlled UψandU†
ψcircuit, and has a total circuit depth of O(k(n+a+T)) . This circuit can be
obtained withO(poly(k,log(˜γ
Nϵ1)))classical time complexity.
Proof. We will begin by considering the domain we require for the polynomial approximation.
Essentially, by noting that if α >1 , the function is being applied to a sub-component of an ℓ2-
normalized vector, and thus the maximum value of its input will be strictly less than1+ϵ 0
α. In some
cases, this could yield a more efficient polynomial approximation, and so we will write our result
both in the setting where the interval of approximation is [−1,1] and[−1+ϵ 0
α,1+ϵ 0
α]. In particular,
33

[[PAGE 34]]
the function will be applied to (⟨0|a⊗In)Uψ|0⟩a+n, and so we must upper-bound the maximum
amplitude in this quantity. Define c∈R such that 0< c≤1 . Define the un-normalized vector
|ϕ⟩n:= (⟨0| a⊗In)Uψ|0⟩a+n. Define |Eψ⟩n:=|ψ⟩ n−α|ϕ⟩ n, and note that ∥|ϕ⟩n∥2≤1, and thus
that1
α∥|ψ⟩ n− |E ψ⟩n∥2≤1. Additionally, by Definition 3, ∥|Eψ⟩n∥2≤ϵ0. Define {ϕj}jsuch that
|ϕ⟩n=PN
j=1ϕj|j⟩n. Thus,|ϕ j| ≤ ∥|ϕ⟩ n∥2≤1
α(1 +ϵ 0). Definec:= min(1
α(1 +ϵ 0),1).
LetNψ:=N . LetNP:=∥P(|ψ⟩/α)∥2. Define the degree k−1 polynomial Q(x) :=P(x)/x ,
and defineϵ 2such thatmax x∈[−c,c] |P(x)−f(x)| ≤ϵ 2.
Using Lemma 6 of Rattew and Rebentrost [58], we can get a (1, a+n+ 2,0) -block-encoding
UAofA:=diag(U ψ|0⟩a+n)withO(a+n) circuit depth, and 6additional calls to a controlled
Uψcircuit. Invoking Theorem 56 of Gilyén et al. [36] withQ(x)/4˜γ , we get the unitary UQ, a
(1, a+n+ 4, δ) -block-encoding for Q(A)/4˜γ , requiring O((a+n)k) single and two-qubit gates,
O(k) calls to a controlled UAcircuit, and O(poly(k,log(1/ϵ))) classical computation to determine
the QSVT rotation angles to implement the degree kpolynomial. We can equivalently call UQa
(1, a+n+ 4,0) -block-encoding for some matrix V, such that ∥V−Q(A)/4˜γ∥2≤δ. Additionally,
define EQ:=V−Q(A)/4˜γ . Since for any vector x,Q(diag(x))x=P(x) , we get, V Uψ|0⟩a+n=
P(Uψ|0⟩a+n)
4˜γ+EVUψ|0⟩a+n. Additionally, noting that (⟨0|a⊗In)P(x) =P((⟨0| a⊗In)x), and that
(⟨0|a⊗In)Uψ|0⟩a+n=|ϕ⟩ n, we get (⟨0|a⊗In)V U ψ|0⟩a+n=P(|ϕ⟩ n)
4˜γ+ (⟨0| a⊗In)EVUψ|0⟩a+n.
Define ˜Uψ:=I a+n+4 ⊗Uψ.
First, note that ˜Uψ|0⟩2a+2n+4 = (|0⟩ n+a+4 ⊗Ia+n)Uψ|0⟩a+n. Then, note that (⟨0|n+2a+4 ⊗
In) = (⟨0| a⊗In)(⟨0| n+a+4 ⊗Ia+n). Consequently, by Definition 2, since (⟨0|n+a+4 ⊗
Ia+n)UQ(|0⟩n+a+4 ⊗Ia+n) =V,
(⟨0|n+2a+4 ⊗In)UQ˜Uψ|0⟩2n+2a+4 = (⟨0| a⊗In)V U ψ|0⟩a+n (B.72)
=P(|ϕ⟩ n)
4˜γ+ (⟨0| a⊗In)EVUψ|0⟩a+n.(B.73)
We will now show thatU Q˜Uψis a VE for1
Nψf(|ψ⟩ n/α). Precisely, we must upper-bound,
ξ1:=1
Nψf(|ψ⟩ n/α)−4˜γ
Nψ(⟨0|n+2a+4 ⊗In)UQ˜Uψ|0⟩2n+2a+4
2(B.74)
≤1
Nψ 
∥f(|ψ⟩ n/α)−P(|ϕ⟩ n)∥2+ 4˜γ∥(⟨0| a⊗In)EVUψ|0⟩a+n∥2
.(B.75)
Let⟨j|E ψ⟩:=e j. We will now prove a sequence of simple facts. Since |f(x)−f(x+b)| ≤
L|b|, and using |ϕ⟩n=|ψ⟩n−|Eψ⟩n
α, we have that ∥f(|ψ⟩ n/α)−f(|ϕ⟩ n)∥2
2=PN
i=j|f(ψ j)−
f((ψ j−ej)/α)|2≤L2
α2PN
j=1|ej|2=L2
α2∥|Eψ⟩n∥2
2≤L2ϵ0
α2. Then, ∥f(|ϕ⟩ n)−P(|ϕ⟩ n)∥2
2=PN
j=1|f(ϕ j)−P(ϕ j)|2≤max x∈[−c,c] |f(x)−P(x)|2N≤ϵ2
2N. Then,
∥f(|ψ⟩ n/α)−P(|ϕ⟩ n)∥2=∥f(|ψ⟩ n/α)−f(|ϕ⟩ n) +f(|ϕ⟩ n)−P(|ϕ⟩ n)∥2(B.76)
≤Lϵ0
α+ϵ2√
N.(B.77)
At this point, the proof branches into two cases. The first case is where we simply use the uniform
approximation to the function on the entire interval [−1,1] . The second case, which should only be
used when approximating the function on [−τ, τ] yields a better asymptotic approximation, will be
proven after.
Noting that∥(⟨0| a⊗In)EVUψ|0⟩a+n∥2≤δ, we can now get the overall bound of
ξ1≤1
NψLϵ0
α+ϵ2√
N+ 4˜γδ
≤1
Nψ
Lϵ0+ϵ2√
N+ 4˜γδ
.(B.78)
Thus, we have shown that UQ˜Uψis a(4˜γ
Nψ,2a+n+4,1
Nψ(Lϵ0+ϵ2√
N+4˜γδ)) -VE for1
Nψf(|ψ⟩ n/α).
To get the overall error-bound, we will set ϵ2√
N=Lϵ 1/2, and 4˜γδ=Lϵ 1/2, yielding ϵ2=Lϵ1
2√
N,
andδ=Lϵ1
8˜γ. This gives a (4˜γ
Nψ,2a+n+ 4,L
Nψ(ϵ0+ϵ1))-VE for1
Nψf(|ψ⟩ n/α), and requires O(k)
34

[[PAGE 35]]
calls to a controlled UψandU†
ψcircuit, and has a total circuit depth of O(k(n+a+T)) . This circuit
can be obtained withO(poly(k,log(˜γ
Lϵ1)))classical time complexity.
To make the preceding result easier to use, we provide a special case for transformation by the error
function, and again do not claim novelty.
Lemma B.19(Application of erf(νx) to a Vector Encoding).Let N= 2n, letν≥1/2 , let1≥ϵ 0≥
0and let 0< ϵ 1≤2. We are given a unitary matrix Uψwith circuit complexity O(T) which is an
(α, a, ϵ 0)-VE for the n-qubit quantum state |ψ⟩n, and we are also given the error function fν(x) =
erf(νx) . LetN:=∥f ν(|ψ⟩n/α)∥2. Then, we can obtain a
16ν√πN,2a+n+ 4,2να(ϵ 0+ϵ1)
-VE
forfν(|ψ⟩n/α)/N , with O(νlog(√
N
ϵ1))queries to a controlled UψandU†
ψcircuit, and with a total
circuit depth ofO(νlog(√
N
ϵ1)(a+n+T)). Moreover,N ≥1
2α.
Proof.From Lemma F.1, we know that the Lipschitz constantLoferf(νx)isL=2ν√π.
Define c=O(1/α) . Using Lemma F.1, we can obtain a degree k∈O(νlog(ν/αϵ′))polynomial
Pk,νsuch that Pk,ν(0) = 0 andmax x∈[−c,c] |Pk,ν(x)−f ν(x)| ≤ϵ′. Since we need ϵ′≤Lϵ1
2√
N,
we can set ϵ′=νϵ1
10√
Nin accordance with Lemma B.18, we have a degree k∈O(νlog(√
N
ϵ1))
polynomial approximation.
From Lemma F.1, for ν≥1/2 , we know that ∀x∈[−1,0)∪(0,1],|erf(νx)| ≥ |x/2| . Consequently,
N2=PN
j=1|f(ψ j/α)|2≥(1
2α)2. Additionally, we know that ˜γ= max x∈[−1,1] |Pk,ν(x)/x| ≤4ν√π.
Invoking Lemma B.18, setting with all of the above facts and setting ˜γ=4ν√πthen gives the
complexity.
We will now formally define our ℓ2-norm squared pooling; this is essentially just an ℓ2-norm pooling
operation followed by an element-wise square. Throughout we will assume that dimensions neatly
line-up, noting that if they don’t padding can be used to easily and efficiently ensure alignment.
Definition B.7(Squared ℓ2Norm Pooling).Given an N-dimensional vector |ϕ⟩=PN
j=1ϕj|j⟩, and
a positive integer Csuch that Nis divisible by C, define fj:= (j−1)N
C+1. Then, we define ℓ2-norm
squared pooling by poolC(|ϕ⟩) :=PC
j=1PjN
C
l=fjϕ2
l|j⟩, where {|j⟩} is the set of C-dimensional basis
vectors.
Lemma B.20(Error Propagated Through ℓ2Norm Squared Pooling).Define the N-dimensional
vectors |ϕ⟩and|˜ϕ⟩, such that|ϕ⟩ − | ˜ϕ⟩
2≤ϵ. Then, defining a positive integer Csuch that Nis di-
visible by C, and defining poolCas per Definition B.7, we have thatpoolC(|ϕ⟩)−poolC(|˜ϕ⟩)
2≤
2Nϵ√
C.
Proof. Let|ϕ⟩=PN
j=1ϕj|j⟩, and let |˜ϕ⟩=PN
j=1˜ϕj|j⟩. Then,|ϕ⟩ − | ˜ϕ⟩
2≤ϵ implies that
∀j,|ϕ j−˜ϕj| ≤ϵ. Then, additionally using that|ϕ j+˜ϕj| ≤2,
poolC(|ϕ⟩)−poolC(|˜ϕ⟩)2
2=CX
j=1
jN/CX
l=fj(ϕl−˜ϕl)(ϕl+˜ϕl)
2
≤4CX
j=1Nϵ
C2
=4N2ϵ2
C.
(B.79)
C GENERALARCHITECTURALBLOCKS
The architectural blocks we present in this paper are intended to demonstrate how the different
operations on encoded matrices and vectors can be combined to coherently implement various
35

[[PAGE 36]]
architectures on quantum computers. There is a rich set of possibilities, and we are only exploring a
small but elucidating set.
Two of the most important concepts governing the complexity of the quantum implementation of any
classical architecture are: (1) the number of non-linear activation layers, and (2) the ℓ2norm of the
vectorized input tensor as it propagates through the network.
In order for a unitary matrix (a linear operator) to enact a non-linear transformation on a vector,
its definition must depend on the vector it is being applied to. Consequently, techniques which
enact non-linear transformations on state-amplitudes (e.g., Rattew and Rebentrost [58], Guo et al.
[96]) must have circuit definitions which depend on the vector-encoding circuit they are being
applied to. Thus, if the unitary circuit implementing the transformation requireseven twocalls to
the input vector encoding, then the circuit complexity will grow exponentially with the number of
non-linear activations. Consequently, wide but shallow multi-layer architectures are ideal for quantum
acceleration. Finally, an alternative to fully coherent quantum acceleration is to periodically read-out
the vector in intermediate layers of the network. As discussed in the introduction, several quantum
computing papers have proposed this approach. However, in general, since reading out a quantum
state incurs a dimension-dependent cost [ 97,98] (and incurs polynomial error-dependence) this
either imposes significant constraints on the types of architectures that can be accelerated (requiring
frequent mappings to very low-dimensional spaces where readout is cheaper), or incur asymptotically
dominating error accumulation. Nevertheless, there are certain settings where periodic state readout
may be desirable, and our techniques are fully compatible with these ideas.
The second key concept governing the complexity of a quantum implementation of an architecture
relates to the norm of the encoded vector as it propagates through the network. Whenever a sample is
drawn from an encoded vector, a cost inversely proportional to the norm of the encoded vector must
be paid. Similarly, whenever an encoded vector is normalized, an inverse norm-cost must be paid.
Consequently, to obtain provable end-to-end complexity results, we need to be able to lower-bound
the norm of the encoded vector whenever we apply a layer norm (or draw a sample from the output
of the network). A key tool in doing this is the skip connection, as it allows the norm from the
previous layer to be preserved in the output of the next layer. Additionally, if the weight layers are
normalized (i.e., if Wrepresents the matrix form of any parameter layer, then ∥W∥2≤1), and the
activation function is scaled so that its Lipschitz constant on the interval [−1,1] is at most 1, this
results in provable norm-preservation bounds. Requiring weight-layers to be sub-normalized has been
extensively explored in the classical deep learning literature [ 99,100,101], as sub-normalization can
help prevent network norm explosion as deeper networks are trained.
It is worth briefly noting that, in certain cases, the sub-normalization condition on the weight layers
can be removed (i.e., for matrix W,0≤ ∥W∥2≤cwhere c≥1 ). This is done by implementing
W/∥W∥2, and then scaling the input of the subsequent activation function by ∥W∥2. If using the
error function activation, this increases the cost of the polynomial approximation by an amount
proportional to c. We do not consider this regime as it makes it more challenging to prove norm
preservation properties after the skip connection, but stress that quantum computers can actually
implement such regimes. Numerical studies examining norm preservation for such networks could
shed light into their efficiency.
Proof of Lemma 6. The parameter κin the lemma is designed for situations where we don’t have a
perfect block-encoding of the matrix we would like. For instance, in cases where we want to apply
some matrix A, but we are only able to get a block-encoding of A/2. We can fix this when applying
the activation function by scaling its input to remove the1/2factor.
Letν:= 4κ/5 . Let|ϕ1⟩n:=W|ψ⟩ n/κ,N1:=∥|ϕ 1⟩n∥2, and|Φ1⟩n:=|ϕ 1⟩n/N1. Using Lemma 2
we getU 1a(N−1
1, a+b, ϵ 0N−1
1)-VE for|Φ 1⟩nwithO(T 1+T2)circuit complexity.
Let|ϕ2⟩n:=f(W|ψ⟩ n)),N2:=∥|ϕ 2⟩n∥2, and |Φ2⟩n:=|ϕ 2⟩n/N2. Define 0< ϵ 1≤
1. Invoking Lemma B.19 with U1andf(κx) = erf(4κx/5) = erf(νx) , we obtain U2a
16ν√πN2,2(a+b) +n+ 4,2νN−1
1(ϵ0+ϵ1)
-VE for f(|Φ 1⟩nN1)/∥f(|Φ 1⟩nN1)∥2=|Φ 2⟩n.U2
has circuit complexityO(νlog(√
N
ϵ1)(a+b+n+T 1+T2)).
36

[[PAGE 37]]
Figure 4:Full-rank linear-pooling output block.
Figure 5: This figure shows the final output architectural block used in our neural networks for
Regimes 1 and 2. Here, g(x) =x2andWis a sub-normalized (potentially full-rank and dense)
matrix.
So as to invoke Lemma 1 to implement the skip connection and obtain a state proportional to |ψf⟩n,
we will need to factor out a common factor of√π
16ν. Consequently, we invoke Lemma B.9 on Uψto
obtainU′
ψa(16ν√π, a+ 2, ϵ 0)-VE for|ψ⟩ nwithO(T 1+a)circuit complexity.
Define |γ⟩n:=√π
32ν(|ψ⟩n+|Φ 2⟩nN2) =√π
32ν(|ψ⟩n+f(W|ψ⟩ n)),Nγ:=∥|γ⟩ n∥2and|Γ⟩n:=
|γ⟩n/Nγ. Then, we can invoke Lemma 1 (setting τ= 1/2 ) with U′
ψandU2, yielding U3a
N−1
γ,2(a+b) +n+ 5, N−1
γ[ϵ0√π
16ν+N2√π
16ν(2νN−1
1(ϵ0+ϵ1))]
-VE for |Γ⟩n, with circuit com-
plexity O(νlog(√
N
ϵ1)(a+b+n+T 1+T2)). We will now simplify the error component of this VE
statement.
First, define |x⟩n=P
ixi|i⟩n=W|ψ⟩ n. Then, using the fact that f(x) = erf(4x/5) has
a Lipschitz-constant of8
5√π(as per Lemma B.19), N2
2=∥f(W|ψ⟩ n)∥2
2≤P
i|f(x i)|2≤
(8
5√π)2P
i|xi|2= (8
5√π)2∥W|ψ⟩ n∥2
2. Since ∥W∥2≤1,∥W|ψ⟩ n∥2≤1, and thus N2≤
8
5√π≤0.91 . Next, we must lower-bound Nγ. Thus, N2
γ= (√π
32ν)2 
1 +N2
2+ 2N 2⟨Φ2|ψ⟩
≥
(√π
32ν)2(1− N 2)2≥(√π
32ν)2(0.09)2. Consequently, using that ν≤8/5 (since κ≤2 ) we get
thatNγ≥1/400 . Additionally, it is straight-forward to show that N2/N1≤0.91κ≤2 .
Inserting all of these values and performing simple algebra, we find that U3is equivalently a  
N−1
γ,2(a+b) +n+ 5,355(ϵ 0+ϵ1)
-VE for|Γ⟩ n.
Let0< ϵ 2≤1. Then, invoking Lemma B.8, we get Uf, a(1,2(a+b)+n+9,2(355(ϵ 0+ϵ1)+ϵ 2))-
VE for |Γ⟩n, with circuit complexity O(log(√
N
ϵ1) log(1
ϵ2)(a+b+n+T 1+T2)). If we let ϵ2=ϵ1,
then we can simplify this to a (1,2(a+b) +n+ 9,712(ϵ 0+ϵ1))-VE with circuit complexity
O(log(√
N
ϵ1) log(1
ϵ1)(a+b+n+T 1+T2)).
Proof of Lemma 7. This result comes from repeatedly invoking Lemma 6, with the output of each
application becoming the input of the next.
We will first give a bound on the total number of ancilla qubits of the block-encoding giving the
final output after kresidual block layers. Let a0=a.c= 2b+n+ 9 . After one application of
the residual block, the number of ancillas is given by a1= 2a 0+c. Then, the general form for the
number of ancillas is given by the recurrence ai= 2a i−1+c. We can obtain an upper-bound by
instead setting ai= 2(a i−1+c). Clearly, ai= 2i(a+c) = 2i(a+ 2b+n+ 9) . Thus, we have a
(1,2k(a+ 2b+n+ 9), ϵ)-block-encoding.
We will now determine a bound on the resulting error, ϵ. Note that the ithresidual block introduces
a new error-parameter ϵiwhich controls the error in the activation function and the normalization
of that block. After the first iteration, the error δ1is given by δ1= 712(ϵ 0+ϵ1). After the second
iteration, the error from the previous iteration becomes the new ϵ0, and so the error after the second
iteration is given by δ2= 712(δ 1+ϵ2). We can set ϵi=δi−1, giving the general form of the error
after the ithresidual layer of δi= 1424δ i−1= 724·1424i−1ϵ1= 1424iϵ1/2. Noting that we want
37

[[PAGE 38]]
a final error of at most ϵ, we must set δk≤ϵ. I.e., we can set ϵ= 1424kϵ1/2 =⇒ϵ 1= 2ϵ/1424k.
Thus, fori >1, eachϵ i=δi−1= 1424iϵ1/2 =1424i
1424kϵ=ϵ/1424k−i.
Define h(ϵi) := log(√
N/ϵ i) log(1/ϵ i). Let the circuit complexity of the block-encoding after
applying iresidual blocks be O(R i). Noting that R1∈O(h(ϵ 1)(a0+b+n+T 1+T2)),Ri
asymptomatically dominates T1, T2, ai−1, nandb. Then, the circuit complexity after block i+ 1
will be O(h(ϵ i+1)(ai+b+n+R i+T1))∈O(h(ϵ i+1)(ai+R i)). Then, we can simplify to
find that Rk∈O((a k+R 1)Qk
i=1h(ϵi))∈O((2k(a+ 2b+n) +T 1+T2)Qk
i=1h(ϵ/1424k−i)).
Noting thatQk
i=1h(ϵi)∈O((Qk
i=1log(√
N/ϵ i))2),Qk
i=1h(ϵ/1424k−i))∈O((Qk
i=1(k−i+
log(√
N/ϵ i)))2)∈O((k+log(√
N/ϵ))2k). Since kis an asymptotic constant, O(k+log(√
N/ϵ))∈
O(log(√
N/ϵ)) , and soQk
i=1h(ϵ/1424k−i))∈O(log(√
N/ϵ)2k). Thus, the overall circuit com-
plexity is given byO(log(√
N/ϵ)2k(a+ 2b+n+T 1+T2)).
Lemma C.1(Full-Rank Linear Pooling Output Block).Consider the architecture block shown
in Figure 5. Let the dimension of the input vector be N= 2n, and let the dimension of the output of
the network block be C= 2c(i.e., the number of classes). Let the output of the network be given by
the vector |y⟩c. Suppose we have Uψan(1, a, ϵ 0)-VE for the N-dimensional input vector |ψ⟩n=x
withO(T ϵ0)circuit complexity. Here,T ϵ0makes explicit that the complexity of the input circuit will
be dependent on the desired error of the vector encoding of the layer input to this architectural block.
Suppose we are given access to an arbitrary matrix Wsuch that ∥W∥2≤1 as per Theorem B.1.
Then, if the weight on the skip-path is τ= 0.51 , we can draw a sample from a vector |˜ϕ⟩csuch that|˜ϕ⟩c− |y⟩ c
2≤ϵwithO(log(N√
Cϵ)(Tϵ0+a+n2))circuit complexity and with O(a+n) total
ancilla qubits.
Proof. Letdrepresent the number of bits in part of the QRAM encoding of W, as per Theorem B.1.
Note that dis assumed to be an asymptotic constant. Let |ϕ1⟩n:=Wg(|ψ⟩ n),N1:=∥|ϕ 1⟩n∥and
|Φ1⟩n:=|ϕ 1⟩n/N1. Using Theorem B.1, we can get a (N−1
1,2a+d+ 3 +n,2ϵ 0N−1
1)-VE for
|Φ1⟩nwithO(Tϵ0+dn+n2)circuit complexity. Here dis a constant specifying the precision in the
representation of the elements of the matrix stored as per Definition B.3.
Let|γ⟩ n:=τ|ψ⟩ n+ (1−τ)|Φ 1⟩nN1=τ|ψ⟩ n+ (1−τ)Wg(|ψ⟩ n), and letN γ:=∥|γ⟩ n∥2.
Then, Lemma 1 yields V2a(N−1
γ,2a+d+4+n,3ϵ 0N−1
γ)-VE for |γ⟩n/NγwithO(Tϵ0+dn+n2)
circuit complexity.
We will now lower-bound Nγ. The main idea is that if you are summing two vectors, one with norm
1, and the other with norm at most 1, if you put arbitrarily more mass on the constant-norm vector (δ),
you are guaranteed that the vectors cannot fully cancel out, and thus that some norm is preserved in the
sum. Note that N1=∥Wg(|ψ⟩ n)∥2≤ ∥W∥2P
jψ2
j|j⟩n
2≤P
jψj|j⟩n
2= 1. Consequently,
|⟨ψ|Φ 1⟩| ≤1, and so
N2
γ=∥τ|ψ⟩ n+ (1−τ)|Φ 1⟩nN1∥2
2=τ2+ (1−τ)2N2
1+ 2τ(1−τ)N 1⟨ψ|Φ 1⟩(C.1)
≥τ2+ (1−τ)2N2
1+ 2τ(1−τ) = (τ−(1−τ)N 1)2.(C.2)
For some parameterδ∈[0,1], assuming thatτ= (1 +δ)/2, we then get thatN γ≥δ.
Then, define ϵ1∈(0,1] . We can then invoke Lemma B.8 yielding V3a(1,2a+d+8+n,6ϵ0
δ+2ϵ 1)-
VE for|γ⟩ n/NγwithO(1
δlog(1/ϵ 1)(Tϵ0+a+dn+n2))circuit complexity.
Define poolCas per Definition B.7. Noting that poolC(|γ⟩n/Nγ) =|y⟩ c.
We can equivalently define some ℓ2-normalized state |˜Γ⟩nsuch that V3is a(1,2a+d+ 8 +n,0) -VE
for|˜Γ⟩n. Then, since|˜Γ⟩n−|γ⟩n
Nγ
2≤6ϵ0
δ+ 2ϵ 1, we can invoke Lemma B.20 which shows thatpoolC(|˜Γ⟩n)− |y⟩ c
2≤2N√
C(6ϵ0
δ+ 2ϵ 1).
38

[[PAGE 39]]
Consequently, to get an error of at most ϵ, we set2N√
C(6ϵ0
δ+ 2ϵ 1) =ϵ , by setting ϵ1=√
Cϵ
8Nand
ϵ0=ϵ√
Cδ
24N. Then, we can simply draw a sample ϵ-close to |y⟩cinℓ2-norm distance by sampling the
state prepared byV 3and then assigning it to the appropriate bin.
Setting δ= 0.02 gives τ= 0.51 . Then, V3is a(1,2a+d+ 8 +n, ϵ) -VE for |γ⟩n/Nγwith
O(log(N√
Cϵ)(Tϵ0+a+dn+n2))circuit complexity. Consequently, we can draw a sample from some
vector|˜ϕ⟩csuch that|˜ϕ⟩c− |y⟩ c
2≤ϵwithO(log(N√
Cϵ)(Tϵ0+a+dn+n2))∈O(log(N√
Cϵ)(Tϵ0+
a+n2))circuit complexity, and with O(a+n) ancilla qubits, noting that dis an asymptotic
constant.
D FEASIBILITY OFQRAM ASSUMPTIONS
In this section, we consider the feasibility of different QRAM assumptions to help motivate our
discussion in Appendix E. In Section D.1 we consider the feasibility of our QRAM assumptions.
In Section D.2 summarize how arbitrary quantum states can be prepared by using a QRAM data-
structure, in service of our subsequent discussion of the different architectural regimes.
D.1 PASSIVE ANDACTIVEQRAM
It is clear that, if a fault-tolerant quantum computer can be constructed, that a QRAM based on the
various quantum circuit constructions (see Jaques and Rattew [61], Giovannetti et al. [79], Hann
[102] ) can be directly implemented. Moreover, these circuit constructions have log-depth access
costs. However, as laid out in Jaques and Rattew [61], the fundamental issue regarding the practicality
of QRAM comes down to the opportunity cost of the total energy required to implement a query to
the QRAM. Precisely, given a QRAM with Nbits of memory, a QRAM is consideredpassiveif and
only if each query to the QRAM requires o(N) totalenergy input. If the query instead requires Ω(N)
energy input (even if the time complexity is O(polylog(N)) ) then the QRAM isactive. Importantly,
this means that any QRAM implemented in the error-corrected circuit-model must be active, as
each qubit requires O(1) classical resources to run the error-correction, resulting in an Ω(N) total
energy cost per QRAM query. Even if error-correction is not used, if enacting the gates in the system
requires constant energy input (e.g., by enacting the gates as laser pulses) then the QRAM will be
active. If the QRAM is active, then Jaques and Rattew [61] show that a wide-range of quantum linear
algebra applications lose quantum speedup. Moreover, there are additional challenges such as how a
noisy (non-error corrected) quantum memory could be interfaced with an error-corrected quantum
processor.
However, as noted in Jaques and Rattew [61] there is some hope in practice, and we will now outline
their arguments. As an example, consider classical Dynamic Random Access Memory (DRAM).
DRAM requires a constant power draw for each bit in memory, and thus an N-bit memory requires
Ω(N) energy input. This makes DRAM active. Nevertheless, because the energy expenditure of
DRAM is often dwarfed by the energy expenditure of the CPU accessing it, it is usually treated as
being a passive component in classical algorithm design. For instance, Carroll and Heiser [103]
demonstrates that for mobile phones, “RAM power is insignificant in real workloads”, and Mahesri
and Vardhan [104] draws a similar conclusion for laptops. At larger server-scales, the asymptotics
of active memory become more noticeable, but memory still usually draws less power than the
controlling CPU [ 105,106]. Analogously, consider a regime where a QRAM is active, but its
constant energy costs are extremely small relative to the energy costs of the error-corrected quantum
computer it is being interfaced with. Given thesubstantialexpected overheads of quantum error-
correction [ 107], the ratio of energy consumption for an error-corrected QPU to an active QRAM
could be even more favourable than in the classical setting. Then, if there is some way to interface
this noisy device with the error-corrected QPU, for moderate scales (e.g., terabytes of memory), it is
conceivable that the QRAM could be practically treated as passive. We will call this a “practically
passive QRAM”. Nevertheless, even though practically passive QRAMs are asymptotically active,
they are unlikely to allow full error-correction without losing their constant advantages (unless, for
some reason, the structure of QRAM allows for extremely efficient custom-made error-correcting
codes). Consequently, it is important that the QRAM implementation is resilient to errors. Indeed
39

[[PAGE 40]]
QRAMs based on the bucket-brigade architecture [ 77], are intrinsicallyexponentially(in terms of the
number of memory registers) robust to errors [78, 102, 108].
In this paper, for simplicity, when making a QRAM assumption we treat the QRAM as passive.
We stress that substantially more work is needed to fully understand the feasibility of QRAM, but
that it is plausible that the QRAM assumptions made in this paper could be physically realized in
practice. In particular, assuming that truly passive QRAM is impossible, we outline the following
questions (building on Jaques and Rattew [61]) which could result in our results being practically
useful. How can a noisy QRAM system be interfaced with an error-corrected quantum computer? If
such an interface is possible, how do errors in the QRAM propagate through the error-correction in
the QPU? Recent promising work [ 62] provides answers to these two preceding questions, and offers
a path forward for research aiming to construct practically passive and useful QRAM. Additional
questions which need to be investigated to help realize a practically passive QRAM include some
of the following. What is the ratio in energy consumption for plausible practically passive QRAM
systems to the energy consumption of the controlling fault-tolerant QPUs for different error-correcting
codes? Given potential active (practically passive) QRAM architectures, what is the total expected
energy consumption for different sized memories?
D.2 INPUTPREPARATION VIAQRAM
The data-structure due to Kerenidis and Prakash [81] can allow for an arbitrary quantum state to be
prepared, so long as the state amplitudes are made available through a specific QRAM data-structure.
Lemma D.1(Input Data QRAM Data-Structure [ 81]).Let N= 2n. Given a vector x∈RN, we can
define a data-structure utilizing a QRAM with ˜O(N) total qubits7storing x. Then: (1) the cost to
update (insert, delete, or modify) an entry xjisO(n2), (2) using the QRAM data-structure, the state
|x⟩=x/∥x∥2can be prepared by a circuit with depthO(n2), acting onO(n)qubits.
This is just a special case of the more general result in Kerenidis and Prakash [81] giving a similar
data-structure for arbitrary matrices (which we presented as QRAM for quantum data in Appendix A).
Intuitively, the state can be prepared by following Grover-Rudolph [ 82], using the QRAM data
structure containing the tree of binary partial norms of the vector to compute the controlled rotation
angles for each additional qubit.
E ARCHITECTURES INDIFFERENTREGIMES
As summarized in the main text, the results presented thus far can be used to construct a range of
architectures in a number of different settings. In particular, we consider three regimes characterized
by the QRAM assumptions they make. In the first regime, we assume that both the input to the
network and the weights in the network are made available via QRAM. In the second regime, we
assume that the network may use QRAM (since its QRAM data-structure may be pre-computed prior
to inference-time), but that the input to the network is received classically and entirely on-the-fly, and
thus that the input cannot be provided with QRAM (so a cost linear in the dimension of the input
must be paid to load it into the quantum computer). In the third regime, we assume no QRAM. We
will now expand on the arguments presented in the main text in greater detail. To do so, we will first
derive the main end-to-end architectural block which forms the basis of the discussion of our various
architectures (and in particular for the architectures in the first two regimes).
E.1 REGIME1: INPUT ANDNETWORKUSEQRAM
Here we expand on the argument presented in Section 4.1.1.
Online Input ConstructionNoting that as per Section D.2 QRAM data-structures can be efficiently
updated, we note that there are a number of settings where it might be realistic for the input vector to
be provided via QRAM. For example, in any setting where inference needs to be repeatedly performed
on a slowly-changing input (e.g., in an interactive chat with an autoregressive LLM, where each
7Neglecting the finite precision error due to storing vector elements (and their partial squared sums) in binary
representations
40

[[PAGE 41]]
output token becomes part of the new input), or where the input is the result of some other quantum
algorithm. For example, in the context of auto-regressive interactive LLM (where the output would
be a probability distribution over tokens instead of classes), the initial vector xmight be an encoding
of the hidden prompt to the network (and so the associated data-structure can be pre-computed). As a
user queries the LLM, a small number of tokens are added to x, and these updates can be efficiently
performed to the data structure. Then, the network is run, and the new output token is added to x,
again efficiently. This process can then continue to repeat, and so the cost of loading the data is
either entirely precomputed, or amortized on-the-fly. We can envision similar applications in the
classification of video, where a very large, but slowly-changing, video needs to be analysed one frame
at a time. Here, a cost would need to be paid proportional to the number of changing pixels between
each frame, and so the input data-structure could be efficiently updated. Additional settings where it
might be reasonable for the input to be provided efficiently could be if the input corresponds to some
combination of continuous function (via Rattew and Koczor [89]), or if it was prepared as the output
of some other quantum algorithm.
Receptive FieldTo understand the importance of the final linear layer in the architecture for this
regime, we must first summarize the receptive field problem of multi-layer convolutional architectures.
For simplicity, consider a 2D convolution with one input channel and one output channel, and consider
a sequence of ksuch convolutional layers. Let the kernel be D×D . Since a convolutional layer
can map the information in location i, jto, at the furthest, the location i+D, j+D , after klayers
the information in any given entry will come from local information in the input at most ≈kD
pixels away. Consequently, the final layer which is input to the output linear-layer-residual block
will contain features with kDlocal information, which the linear layer then combines in a global
fashion. We conjecture that having a full-rank layer at this stage is more effective for merging the
local information than a similar dimension, but low-rank, linear layer. Since the cost of the quantum
algorithm grows exponentially with depth, without the final linear layer, with such an architecture no
learning could occur which requires global information from the input image.
Moreover, there are other approaches which could be taken to make the local information globally
accessible to the earlier convolutional layers, potentially improving the power of such quantum-
amenable architectures in practice. For instance, after a set number of convolutional layers, a
linear layer could be added to make local information global (however, this damages the nice
algebraic properties of convolutional layers). Alternatively, a sequence of convolutions can be
implemented in each residual block (without activation functions between them) as this would not
increase the complexity exponentially, potentially allowing for many more convolutions in sequence.
Most appealingly, a solution can be found in the popular classical architecture of bilinear neural
networks [ 63] (which forms the basis of the architecture presented for Regime 2). Here, paths
of convolutional-based residual blocks are passed into a Kronecker product, which is followed by
more layers. Via Lemma 3, we can efficiently do this in a quantum computer. Since the Kronecker
product makes all local information globally available, it immediately solves the receptive field
problem. However, while a Kronecker product makes local information globally accessible, it loses
positional information. This can be resolved by enacting a positional encoding along one of the
paths of the network prior to the product, e.g., as is done when Tokenizing the inputs to transformer
architectures [6].
DequantizationA number of quantum algorithms which were believed to have exponential speed-
ups over their classical counterparts lost their exponential speedup after new classical randomized
algorithms were developed which mirrored the quantum input assumptions. For example, see the
works of Kerenidis and Prakash [81] and Tang [109] . Indeed, it seems likely that, as was the case
with the quantum CNN implementation in Kerenidis et al. [41], that the convolutional residual blocks
in our architectures could be dequantized (even though they make no QRAM assumptions). However,
our new techniques enables the final linear-residual block to contain an arbitrary full-rank and dense
matrix. Since known dequantization techniques require the matrix to be either low-rank [ 110,109] or
sparse (with certain strong caveats) [ 111], existing techniques appear insufficient to dequantize our full
architecture. Moreover, as previously discussed, removing the final linear layer introduces receptive
field problems, highlighting that it is not a purely artificial addition to the network. Nevertheless,
it would be interesting to exploring dequantizing the architecture without the final linear layer (or
41

[[PAGE 42]]
perhaps replacing it with a low-rank one), and this could result in some interesting techniques to
classical accelerate inference for certain architectures.
E.2 REGIME2: NETWORKSTORED INQRAM, INPUTLOADEDWITHOUTQRAM
See the discussion in Section 4.1.2.
E.3 REGIME3: NOQRAM
To reiterate, in this regime, both the matrix weights and the network input are not given by QRAM.
We will now prove the complexity of the Regime 3 architecture shown in Figure 1 (c), as discussed
in Section 4.1.3. We note that there are many simple modifications which could be made to this
architecture, for example by having a final low-rank linear layer with O(N) parameters. Adopt the
notation used in Theorem 2. Let the input be a 4×M×M tensor, and define N=M2,n= log2(N),
m= log2(M). Thus, the vectorized input is of dimension O(N) . Letdbe the number of paths into
the input tensor (i.e., the latent dimension will be O(Nd)), as per Figure 1 (c). TXis the access cost
of the input; in the QRAM-free regime we assume a worst-case of TX∈O(N) . LetCbe the number
of output classes (or set of possible output tokens).
Assume d= 2 . Let δ >0 be an error parameter used only in the proof. Directly from the proof
of Theorem 2, we have Uconv, a(1,2k(63 +n), δ) -VE (vector encoding) for the ℓ2-normalized output
of the kconvolutional/residual block layers. UconvhasO(log(N/δ)2k(n2+TX))circuit depth. Note
that in that proof, Ncorresponds to the vectorized dimension of the latent space (i.e., if there is 1 input
and output channel, Ncorresponds to the dimension of the vector acted upon by the matrix-form of
the 2D convolution), and thus corresponds toNdhere.
Let|ϕ⟩represent the exact vector output after the sequence of kconvolutional layers. This VE
corresponds to a state |˜ϕ⟩such that ∥|ϕ⟩ − | ˜ϕ⟩∥2≤δ. Consequently, by Lemma B.20, sampling
this VE (and applying the binning-protocol) yields a sample from a vector poolC(|˜ϕ⟩)such that
∥poolC(|ϕ⟩)−poolC(|˜ϕ⟩)∥ 2≤2N2δ√
C. Noting that the correct output of the network is given by
y=poolC(|ϕ⟩) , we can get an overall error of ϵ, such that the vector we sample from satisfies
∥y−poolC(|˜ϕ⟩)∥ 2≤ϵby setting ϵ=2N2δ√
C=⇒δ=ϵ√
C
2N2. By plugging this into the circuit
complexity of Uconv, and noting that here we assume we pay the full input dimension cost (since there
is no QRAM), TX∈O(N) , and so this simplifies to O(Nlog(N3/ϵ√
C)2k)∈˜O(Nlog(1/ϵ)2k)
total circuit cost. As stated in the main text, since the dimension of the vector acted on by the 2D
convolution is O(N2)(when d=2), the classical cost to compute this is Ω(N2): showinga quadratic
speedup over an exact classical implementation. The speedup can be made asymptotically larger
by increasingd.
Possible LimitationsHere we will outline some of the possible limitations of the architecture
shown in Figure 1 (c). Since there is no final linear layer (in the architecture as directly presented), the
receptive field problems outlined in Section 4.1.1 may appear to apply. However, by virtue of taking
the tensor product of the input paths, local information becomes immediately globally accessible
circumventing this limitation. Moreover, another way that local information could made global is
from the processing that occurs along each path prior to the tensor product, since there is no limit on
the classical processing that can occur (so long as the total compute is linear in the dimension of the
input).
Moreover, our argument against dequantization in the first regime (see Section 4.1.1) relies on the
final dense and full-rank linear layer. However, since this layer is not feasible without QRAM, this
argument does not apply here. However, as we are only suggesting a polynomial speedup in Regime
3, we do not expect a dequantized algorithm to completely close the performance gap past quadratic,
as we benefit from amplitude amplification. However, exploring dequantized algorithms based on the
ideas in this paper appears to be interesting subsequent work.
Finally, if the network only contains the convolutional layers, it will likely be very under-
parameterized making training challenging (see e.g., Allen-Zhu et al. [112] for a discussion on
overparameterized neural networks). However, where the dimension of the vectorized input isN, it
would be easy to add O(N) parameters, either in the classical paths prior to the tensor product, or
42

[[PAGE 43]]
as a final low-rank residual output block (prior to the ℓ2-norm-pooling), so long as the number of
parameters in that block areO(N).
Alternative: Parameterized Quantum Circuits as Network LayersAlternatively, one could
use parameterized quantum circuits as network layers [ 19,21,20], as the number of parameters in
such circuits are usually polylogarithmic in the dimension of the operator. However, such circuits
are often hard to train even on classical machines, due to under-parameterization, the barren plateau
problem [ 24,25], and the exponential amount of bad local minima in the optimization landscape [ 23].
However, given good enough initializations and warm start assumptions [ 113], it may still possible to
train such architectures, leading to potential speed-ups in inference.
Other Possible Sources of SpeedupIn some cases, where the input can be efficiently prepared
without paying a dimension-dependent cost (e.g., the input comes from quantum states which are easy
to prepare, either via some other quantum algorithm, or via techniques like [ 89]) it may be possible
to obtain better than quadratic speedups. However, we leave this as a topic for future investigation.
F TECHNICALRESULTS
We now report a result on the efficient polynomial approximation to the error function due to Low
and Chuang [86], which builds on the results of Sachdeva and Vishnoi [114] . This result is an
improvement over the approximation obtained by an integration of the series expansion for the
Gaussian distribution.
Lemma F.1(Polynomial Approximation to Error Function due to Corollary 4 of Low and Chuang
[86]).Let m≥1/2 ,1≥ϵ >0 . There exists a degree k∈O(mlog(1/ϵ)) polynomial Pk,m(x)such
that
Pk,m(x) :=2me−m2/2
√π
I0(m2/2)x+(k−1)/2X
j=1Ij(m2/2)(−1)jT2j+1(x)
2j+ 1−T2j−1(x)
2j−1

(F.1)
andmax x∈[−1,1] |erf(mx)−P k,m(x)| ≤ϵ . Let1≥c >0 . Alternatively, if k∈O(mlog(mc/ϵ)) ,
thenmax x∈[−c,c] |erf(mx)−P k,m(x)| ≤ϵ . Additionally, for all k,max x∈[−1,1] |Pk,m(x)/x| ≤4m√π,
andPk,m(0) = 0 . Finally, minx∈[−1,1] |erf(mx)/x| ≥1/2 , and erf(mx) has Lipschitz constant
L=2m√π,
Proof. For the case where max x∈[−1,1] |erf(mx)−P k,m(x)| ≤ϵ , the result on the polynomial
approximation is directly taken from Low and Chuang [86]. We will now prove the bound when the
function is constrained to the interval [−c, c] . Let ϵ1:= max x∈[−c,c] |erf(mx)−P k,m(x)|. From
Equation (71) of Corollary 4 of Low and Chuang [86], for a degree kpolynomial approximation, we
have the following error-bound,
ϵ1≤2me−m2/2
√π∞X
j=(k+1)/2Ij(m2/2)(−1)jT2j+1(x)
2j+ 1−T2j−1(x)
2j−1.(F.2)
Using the identity
T2j+1(x)
2j+1−T2j−1(x)
2j−1
= 2Rx
0T2j(t)dt , and using the fact that all Cheby-
shev polynomials of the form T2jare even, we can get the bound that 2T2j+1(x)
2j+1−T2j−1(x)
2j−1≤
2R|x|
0|T2j(t)|dt≤2|x| ≤2 max x∈[−c,c] |x|= 2c, sincemax x∈[−1,1] |T2j(x)| ≤1.
Then, applying the triangle inequality, Equation (F.2) becomes,
ϵ1≤4cme−m2/2
√π∞X
j=(k+1)/2Ij(m2/2).(F.3)
Define ϵgauss,γ,k as per Corollary 3 of Low and Chuang [86]. Define some ϵ′>0 .
Note that ϵgauss,γ,k = 2e−γ2/2P∞
j=n
2+1|Ij(γ2/2)|, and that ϵgauss,γ,k ≤ϵ′ifk∈
43

[[PAGE 44]]
O(p
(γ2+ log(1/ϵ′)) log(1/ϵ′)). Thus, ϵ1≤2cmϵ′
√π. To get an overall error-bound of at most
ϵ, we can set2cmϵ′
√π=ϵ, and so ϵ′=√πϵ
2cm. Thus, if we set k∈O(mlog(cm
ϵ)), we are guaranteed
thatmax x∈[−c,c] |Pk,m(x)−erf(mx)| ≤ϵ.
Next,d
dxerf(mx) =2m√πe−(mx)2, and consequently the maximum value of the derivative of the
function is whenx= 0, i.e.,max x∈[−1,1] |d
dxerf(mx)|=2m√π.
We will now prove that|P k,m(x)/x| ≤4m√πandmin x∈[−1,1] |erf(mx)/x| ≥1/2.
Noting that Pk,m(0) = 0 , (since for x= 0 ,T2j(x) = cos((2j+ 1) arccos(0)) = cos((2j+
1)π/2) = 0 ), by Lipschitz continuity we have that |Pk,m(x)/x| ≤ |d
dxPk,m(x)|. Noting that
d
dx1
2(T2j+1(x)
2j+1−T2j−1(x)
2j−1) =T 2j(x),
max
x∈[−1,1]|Pk,m(x)/x| ≤max
x∈[−1,1]d
dxPk,m(x)(F.4)
= max
x∈[−1,1]2me−m2/2
√π
I0(m2/2) + 2(k−1)/2X
j=1Ij(m2/2)(−1)jT2j(x)
.
(F.5)
A common identity for modified Bessel functions of the first kind states for t̸= 0 ,e1
2y(t+t−1)=P∞
j=−∞tjIj(y). Setting t= 1 , we find ey=P∞
j=−∞Ij(y). Moreover, since Ij(y)≥0 for all
y >0,P(k−1)/2
j=1Ij(m2/2)≤em2/2. Thus, using thatmax x∈[−1,1] |T2j(x)| ≤1,
max
x∈[−1,1]|Pk,m(x)/x| ≤4me−m2/2
√π(k−1)/2X
j=1Ij(m2/2)≤4m√π.(F.6)
Thus, it is clear that this upper-bound is independent of the degree of the polynomial approximation,
and thus applies to the whole intervalx∈[−1,1]and not justx∈[−c, c].
Finally, we must show that minx∈[−1,1] |erf(mx)/x| ≥1/2 . First, note that |erf(mx)/x| is
symmetrical, so we can simply consider the interval x∈[0,1] . Moreover, it is monotonically
decreasing, so we can take the endpoint minx∈[−1,1] |erf(mx)/x|= erf(m) . Since m≥1/2 ,
erf(m)≥erf(1/2)≈0.52>1/2.
44